[[BioInformaticsNote/4Database/README|README]]

# 云计算

**云计算**
- **定义**：云计算是一种商业计算模型，将计算任务分布在大量计算机构成的资源池上，使各种应用系统能够根据需要获取计算力、存储空间和信息服务。
- 技术来源：
	- 并行计算：同时使用多种计算资源解决计算问题的过程
	- 分布式计算：将需要巨大计算能力的问题分成许多小部分进行处理，最后综合结果
	- 网格计算：在动态、多机构参与的虚拟组织中协同共享资源和求解问题
- **特点**：云计算（Cloud Computing）：一种利用大规模低成本运算单元通过IP网络连接，以提供各种计算和存储服务的IT技术
	- 大规模：由具备一定规模的多个节点组成的IT系统，系统规模几乎可以无限扩大。
	- 平滑扩展：系统具备高度的扩展性和弹性，可以以即插即用的方式方便、快速地增加和减少资源。
	- 资源共享：
		- 提供一种或多种形式的计算或存储能力资源池，如物理服务器，虚拟服务器（虚拟机），事务和文件处理能力或任务进程（如并行计算），以及存储能力等。
		- 资源池可通过抽象化方式提供，并能够同时为多种应用提供服务。
	- 动态分配：实现资源的自动分配管理，包括资源即时监控和自动调度等，并能够提供使用量监控和管理。
	- 跨地域：能够将分布于多个物理地点的资源进行整合，提供统一的资源共享，并在各物理地点间实现负载均衡。
- 发展：
	- 2007年：云计算概念诞生，仅过了半年多，其受到的关注程度就超过了网格计算（Grid Computing）
	- 发展路线：并行计算、集群计算、网格计算、云计算
- **与传统计算的区别**
	- **硬件为中心**：传统的计算模式。
	- **软件为中心**：现代的计算模式。
	- **服务为中心**：云计算模式。
		- 数据在云端：不怕丢失,不必备份,可以任意点的恢复 ；
		- 软件在云端：不必下载自动升级 ；
		- 无所不在的计算：在任何时间，任意地点，任何设备登录后就可以进行计算服务；
		- 无限强大的计算：具有无限空间的，无限速度。
- 应用优势：
	- 用户花费低
	- 减少底层管理职责
	- 允许意想不到的资源装载
	- 业务应用实现迅速

服务模式：层级分类
- **IaaS（基础设施即服务，Infrastructure as a Service）**
	- **定义**：
		- 通过互联网获取计算机基础设施方面的服务：计算机、存储空间、网络连接、负载均衡和防火墙等基本计算资源
		- 用户在此基础上部署和运行各种软件，包括操作系统和应用程序
	- **优势**：
		- **低成本**：企业不需要购置硬件，省去了前期的资金投入；使用IaaS服务是按照实际使用量进行收费的，不会产生闲置浪费；IaaS可以满足突发性需求，企业不需要提前购买服务。
		- **免维护**：IT资源运行在IaaS服务中心，企业不需要进行维护，维护工作由云计算服务商承担。
		- **伸缩性强**：IaaS只需几分钟就可以给用户提供一个新的计算资源，而传统的企业数据中心则需要数天甚至更长时间才能完成；IaaS可以根据用户需求来调整资源的大小。
		- **支持应用广泛**：IaaS主要以虚拟机的形式为用户提供IT资源，可以支持各种类型的操作系统，因此IaaS可以支持的应用范围非常广泛。
		- **灵活迁移**：虽然很多IaaS服务平台都存在一些私有的功能，但是随着云计算技术标准的诞生，IaaS的跨平台性能将得到提高。运行在IaaS上的应用将可以灵活地在IaaS服务平台间进行迁移，不会被固定在某个企业的数据中心。
	- 实例
		- 当你想运行成批的程序组，但是没有合适的软硬件环境，可使用Amazon的EC2
		- 当你想在网络上发布一个短期（几天到几个月）的网站，可使用Flexiscale
- **PaaS（平台即服务，Platform as a Service）**
	- **定义**：PaaS是一种分布式平台服务，为用户提供一个包括应用设计、应用开发、应用测试及应用托管的完整的计算机平台。
		- 将软件研发的平台作为一种服务放在网上，加快SaaS开发。
		- 平台包括操作系统、编程语言环境、数据库和 Web 服务器，用户在此平台上部署和运行自己的应用。
		- 用户不能管理和控制底层的基础设施，只能控制自己部署的应用
	- PaaS的主要用户是开发人员，PaaS平台的种类目前较少，比较著名的有
		- Force.com
		- Google App Engine
		- Windows Azure
		- Cloud Foundry
	- **优势**：
		- 友好的开发环境
		- 丰富的服务
		- 精细的管理和控制
		- 弹性强
		- 多租户机制
		- 整合率高
	- 实例
		- 当你想把一个大容量的文件上传到网络上，允许35000个用户使用2个月的时间，可使用Amazon的Cloud Front
		- 当你想在网络上存储大量的文档，但是你没有足够的存储空间，可使用Amazon的S3
- **SaaS（软件即服务，Software as a Service）**
	- **定义**：
		- 通过网络提供软件的模式，用户无需购买软件，而是向提供商租用基于Web的软件，来管理企业经营活动。
		- 云提供商在云端安装和运行应用软件，云用户通过云客户端（通常是 Web 浏览器）使用软件。
		- 云用户不能管理应用软件运行的基础设施和平台，只能做有限的应用程序设置。
	- **优势**：
		- 使用简单：通过浏览器访问，只要有网络，就可以随时随地通过多种设备使用SaaS服务。
		- 支持公开协议：现有的SaaS服务都是基于公开协议的，如HTTM4和HTTM5等。用户只需要使用常用的浏览器就可以使用SaaS服务。
		- 成本低：使用SaaS服务后，用户无需在使用前购买昂贵的许可证，省去了先期投入，只需要在使用过程中按照实际使用付费，成本远远低于桌面版。
		- 安全保障：SaaS服务提供商都提供了比较高级的安全机制，不仅为存储在云端的数据提供加密措施，还通过HTTPS协议确保用户和云平台之间的通信安全。
	- 实现SaaS的关键技术：
		- 大规模多租户支持：它是SaaS模式成为可能的核心技术，运行在应用提供商SaaS上的应用能够同时为多个组织和用户使用，能够保证用户之间的相互隔离。没有多租户技术的支持，SaaS就不可能实现。
		- 认证和安全：认证和安全是多租户的必要条件。当接收到用户发出的操作请求时，其发出请求的用户身份需要被认证，且操作的安全性需要被监控。
		- 定价和计费：定价和计费是SaaS模式的客观要求。提供合理、灵活、具体而便于用户选择的定价策略是SaaS成功的关键之一。
		- 服务整合：它是SaaS长期发展的动力。SaaS应用提供商需要通过与其他产品的整合来提供整套产品的解决方案。
		- 开发和定制：开发和定制是服务整合的内在需要。一般来讲，每个SaaS应用都提供了完备的软件功能，但是为了能够与其他软件产品进行整合，SaaS应用最好具有一定的二次开发功能，包括公开API，提供沙盒以及脚本运行环境等。
	- 实例：CRM、财务计划、HR、文字处理、Email

体系结构
- SOA构建层：封装云计算能力成标准的Web Services服务，并纳入到SOA体系
	- 服务：接口、注册、查找、访问、工作流
- 管理中间件层：云计算的资源管理，并对众多应用任务进行调度，使资源能够高效、安全地为应用提供服务
	- 用户管理：实现云计算商业模式的一个必不可少的环节，包括提供用户交互接口、管理和识别用户身份、创建用户程序的执行环境、对用户的使用进行计费等
	- 任务管理：执行用户或应用提交的任务，包括完成用户任务映象（Image）的部署和管理、任务调度、任务执行、任务生命期管理等
	- 资源管理：均衡使用云资源节点，检测节点故障并试图恢复或屏蔽之，并对资源的使用情况进行监视统计
	- 安全管理：保障云计算设施的整体安全，包括身份认证、访问授权、综合防护和安全审计等
- 资源池层：将大量相同类型的资源构成同构或接近同构的资源池
	- 计算资源池、存储资源池、网络资源池、数据资源池、软件资源池
- 物理资源层：计算机、存储器、网络设施、数据库、软件

网格计算与云计算
- 网格（grid）计算系统：网格是为了满足对地理位置分散的资源进行集成、共享而发展起来的。网格的概念起源于元计算(即将分布的资源作为一个整体来使用) ，网格从连接超级计算中心开始，发展到连接互联网上的各类资源，包括各种计算设备、实验设备、存储器、数据和应用程序等。网格中的资源在物理上往往是分布的，但在逻辑上是共享的。网格资源具有明显的动态性和多样性，会动态增加或动态减少，资源的种类是异构且多样的。
	- 网格
		- 将网络上分布的资源聚合为一体，建立计算和数据处理的通用系统支撑平台，实现资源共享和联合工作
			- 资源：计算、数据、存储、设备、软件和人员等
		- 网格计算能够对位于分布的计算资源和数据资源虚拟化，例如处理器、网络带宽、存储能力等，从而创建出一个单一系统映像，保证用户和应用程序能够无缝地访问巨大的IT能力
- 网格计算的目的：通过任何一台计算机都可以提供无限的计算能力，可以接入浩如烟海的信息。网格计算的 主要目的包括：
	- 资源集中 —— 使公司用户能够将公司的整个 IT 基础设施看作是一台计算机，能够根据他们的需要找到尚未被利用的资源。
	- 数据共享 —— 使各公司接入远程数据。这对某些生命科学项目尤其有用，因为在这些项目中，各公司需要和其他公司共享人类基因数据。
	- 通过网格计算来合作 —— 使广泛分散在各地的组织能够在一定的项目上进行合作，整合业务流程，共享从工程蓝图到软件应用程序)等所有信息，协同处理项目中的问题。
	- 有质量的服务(QoS)——是指能针对不同用户或者不同数据流采用相应不同的优先级，或者是根据应用程序的要求，保证数据流的性能达到一定的水准,为同一网络中的各结点提供质量有保障的服务。
- **特点**：
	- **资源共享**：多个节点之间的资源共享。
	- **协作计算**：多个节点协同完成计算任务。
- 概念比较
	- **资源管理**：网格计算强调资源共享和协作计算，而云计算更注重资源的动态分配和管理。
	- **服务模式**：网格计算主要提供计算资源，而云计算提供多层次的服务（IaaS、PaaS、SaaS）。
- Gloud（云格）=Grid+Cloud
	- 网格计算与云计算的关系，就像是OSI与TCP/IP之间的关系
	- 云计算是网格计算的一种简化形态，网格不仅要集成异构资源，还要解决许多非技术的协调问题，也不像云计算有成功的商业模式推动，所以实现起来要比云计算难度大很多。但对于许多高端科学或军事应用而言，云计算是无法满足需求的，必须依靠网格来解决。
	- 不久的将来，建立在云计算之上的“商业2.0”与建立在网格计算之上的“科学2.0”都将取得成功。
	- 网格技术主要解决分布在不同机构的各种信息资源的共享问题，而云计算主要解决计算力和存储空间的集中共享使用问题。

云计算的发展和应用
- **云计算的发展趋势**
	- **技术成熟**：云计算技术逐渐成熟，越来越多的企业和个人开始使用云计算服务。
	- **行业应用**：云计算在各个行业中的应用越来越广泛，如金融、医疗、教育、政府等。
- 应用案例**
	- **Amazon EC2**：提供弹性的计算能力，支持大规模的计算任务。
		- 研发了弹性计算云EC2（Elastic Computing Cloud）和简单存储服务S3（Simple Storage Service）为企业提供计算和存储服务
		- 收费的服务项目包括存储空间、带宽、CPU资源以及月租费
		- 诞生不到两年的时间内，Amazon的注册用户就多达44万人，其中包括为数众多的企业级用户
	- **Google App Engine**：提供一个平台，让用户在其上开发和运行Web应用。
		- Google搜索引擎建立在分布在30多个站点、超过200万台服务器构成的云计算设施的支撑之上，这些设施的数量正在迅猛增长
		- Google的一系列成功应用，包括Google地球、地图、Gmail、Docs等也同样使用了这些基础设施
		- 目前，Google已经允许第三方在Google的云计算中通过Google App Engine运行大型并行应用程序
		- Hadoop模仿了Google的实现机制
	- Microsoft
		- 微软于2008年10月推出了Windows Azure操作系统。Azure(译为“蓝天”)是继Windows取代DOS之后，微软的又一次颠覆性转型。
		- 微软在2010年10月的PDC大会上，公布了Windows Azure云计算平台的未来蓝图，跳出单纯的基础架构作服务的框架，将Windows Azure定位为平台作服务
	- **Salesforce.com**：提供CRM（客户关系管理）的SaaS服务。

云计算压倒性的成本优势：节约总成本>30倍
- 云计算成本：下降5-7倍
	- 硬件成本
	- 电价
	- 管理费用
- 云计算资源利用率：下降5-7倍
	- 提供弹性的服务，超大资源池中动态分配和释放资源
	- 资源利用率达到80%左右，是传统模式5～7倍

# Google云计算原理与数据库技术

Google文件系统GFS
- GFS设计动机：
	- Google需要一个支持海量存储的文件系统
		- 购置昂贵的分布式文件系统与硬件？
		- 是否可以在一堆廉价且不可靠的硬件上构建可靠的分布式文件系统？
	- 为什么不使用当时现存的文件系统？
		- Google所面临的问题与众不同
		- 不同的工作负载，不同的设计优先级（廉价、不可靠的硬件）
		- 需要设计与Google应用和负载相符的文件系统
- 系统架构：
	- GFS将容错的任务交给文件系统完成，利用软件的方法解决系统可靠性问题，使存储的成本成倍下降。GFS将服务器故障视为正常现象，并采用多种方法，从多个角度，使用不同的容错措施，确保数据存储的安全、保证提供不间断的数据存储服务
		- Client（客户端）：应用程序的访问接口
		- Master（主服务器）：管理节点，在逻辑上只有一个，保存系统的元数据，负责整个文件系统的管理
		- Chunk Server（数据块服务器）：负责具体的存储工作。数据以文件的形式存储在Chunk Server上
	- 特点：
		- 实现机制：
			- 客户端首先访问Master节点，获取交互的Chunk Server信息，然后访问这些Chunk Server，完成数据存取工作。这种设计方法实现了控制流和数据流的分离。
			- Client与Master之间只有控制流，而无数据流，极大地降低了Master的负载。
			- Client与Chunk Server之间直接传输数据流，同时由于文件被分成多个Chunk进行分布式存储，Client可以同时访问多个Chunk Server，从而使得整个系统的I/O高度并行，系统整体性能得到提高。
		- 采用中心服务器模式
			- 可以方便地增加Chunk Server
			- Master掌握系统内所有Chunk Server的情况，方便进行负载均衡
			- 不存在元数据的一致性问题
		- 不缓存数据
			- 文件操作大部分是流式读写，不存在大量重复读写，使用Cache对性能提高不大
			- Chunk Server上数据存取使用本地文件系统，若读取频繁，系统具有Cache
			- 从可行性看，Cache与实际数据的一致性维护也极其复杂
		- 在用户态下实现
			- 利用POSIX编程接口存取数据降低了实现难度，提高通用性 
			- POSIX接口提供功能更丰富
			- 用户态下有多种调试工具
			- Master和Chunk Server都以进程方式运行，单个进程不影响整个操作系统
			- GFS和操作系统运行在不同的空间，两者耦合性降低
		- 只提供专用接口
			- 降低实现的难度
			- 对应用提供一些特殊支持
			- 降低复杂度
- 容错机制
	- Master容错
		- Master包括：Name Space（文件系统目录结构）、Chunk与文件名的映射、Chunk副本的位置信息（默认有三个副本）
		- 单个Master，对于前两种元数据，GFS通过操作日志来提供容错功能
		- 第三种元数据信息保存在各个Chunk Server上，Master故障时，磁盘恢复
		- GFS还提供了Master远程的实时备份，防止Master彻底死机的情况
	- 采用副本方式实现Chunk Server容错
		- 每一个Chunk有多个存储副本（默认为三个），分布存储在不同的Chunk Server上用户态的GFS不会影响Chunk Server的稳定性
		- 副本的分布策略需要考虑多种因素，如网络的拓扑、机架的分布、磁盘的利用率等
		- 对于每一个Chunk，必须将所有的副本全部写入成功，才视为成功写入
		- GFS中的每一个文件被划分成多个Chunk，Chunk的默认大小是64MB
		- Chunk Server存储的是Chunk的副本，副本以文件的形式进行存储
		- 每个Chunk又划分为若干Block（64KB），每个Block对应一个32bit的校验码，保证数据正确（若某个Block错误，则转移至其他Chunk副本）
- 系统管理技术
	- 大规模集群安装技术：GFS集群中通常有非常多的节点，需要相应的技术支撑
	- 故障检测技术：GFS构建在不可靠廉价计算机之上的文件系统，由于节点数目众多，故障发生十分频繁
	- 节点动态加入技术：新的Chunk Server加入时 ，只需裸机加入，大大减少GFS维护工作量
	- 节能技术：Google采用了多种机制降低服务器能耗，如采用蓄电池代替昂贵的UPS

分布式数据处理MapReduce
- 产生背景：
	- 概念：
		- 一种处理海量数据的并行编程模式，用于大规模数据集（通常大于1TB）的并行运算
		- “Map（映射）”、“Reduce（化简）”的概念和主要思想，都是从函数式编程语言和矢量编程语言借鉴
		- 适合非结构化和结构化的海量数据的搜索、挖掘、分析与机器智能学习等
	- Google拥有海量数据，并且需要快速处理
		- 计算问题简单，但求解困难：简单的问题，计算并不简单！
			- 待处理数据量巨大（PB级），只有分布在成百上千个节点上并行计算才能在可接受的时间内完成
			- 如何进行并行分布式计算？
			- 如何分发待处理数据？
			- 如何处理分布式计算中的错误？
		- Jeffery Dean设计一个新的抽象模型， 封装并行处理、容错处理、本地化计算、负载均衡的细节，还提供了一个简单而强大的接口这就是MapReduce
- 编程模型
	- MapReduce运行模型：开发者需编写两个主要函数
		- Map函数——对一部分原始数据进行指定的操作。每个Map操作都针对不同的原始数据，因此Map与Map之间是互相独立的，这使得它们可以充分并行化
			- Map输入参数：in_key和in_value，它指明了Map需要处理的原始数据
			- Map输出结果：一组<key,value>对，这是经过Map操作后所产生的中间结果
		- Reduce操作——对每个Map所产生的一部分中间结果进行合并操作，每个Reduce所处理的Map中间结果是互不交叉的，所有Reduce产生的最终结果经过简单连接就形成了完整的结果集
			- Reduce输入参数：（key, [value1,…,valuem]）
			- Reduce工作：对这些对应相同key的value值进行归并处理
			- Reduce输出结果：（key, final_value），所有Reduce的结果并在一起就是最终结果
	- 怎么用MapReduce计算一个大型文本文件中各单词出现次数？
		- Map的输入参数指明了需要处理哪部分数据，以“<在文本中的起始位置，需要处理的数据长度>”表示，经过Map处理，形成一批中间结果“<单词，出现次数>”。而Reduce函数处理中间结果，将相同单词出现的次数进行累加，得到每个单词总的出现次数
- 实现机制
	- 操作过程
		1. 输入文件分成M块，每块大概16M～64MB（可以通过参数决定），接着在集群的机器上执行分派处理程序
		2. M个Map任务和R个Reduce任务需要分派，Master选择空闲Worker来分配这些Map或Reduce任务
		3. Worker读取并处理相关输入块，Map函数产生的中间结果<key,value>对暂时缓冲到内存
		4. 中间结果定时写到本地硬盘，分区函数将其分成R个区。中间结果在本地硬盘的位置信息将被发送回Master，然后Master负责把这些位置信息传送给Reduce Worker
		5. 当Master通知执行Reduce的Worker关于中间<key,value>对的位置时，它调用远程过程，从Map Worker的本地硬盘上读取缓冲的中间数据。当Reduce Worker读到所有的中间数据，它就使用中间key进行排序，这样可使相同key的值都在一起
		6. Reduce Worker根据每一个唯一中间key来遍历所有的排序后的中间数据，并且把key和相关的中间结果值集合传递给用户定义的Reduce函数。Reduce函数的结果写到一个最终的输出文件
		7. 当所有的Map任务和Reduce任务都完成的时候，Master激活用户程序。此时MapReduce返回用户程序的调用点
	- MapReduce容错
		- Master会周期性地设置检查点（checkpoint），并导出Master的数据。一旦某个任务失效，系统就从最近的一个检查点恢复并重新执行
		- Master周期性地给Worker发送ping命令，若没有应答，则认为Worker失效，终止其任务调度，把该任务调度到其他Worker上重新执行
- 案例分析：
	- 假设有一批海量的数据，每个数据都是由26个字母组成的字符串，原始的数据集合是完全无序的，怎样通过MapReduce完成排序工作，使其有序（字典序）呢？（排序通常用于衡量分布式数据处理框架的数据处理能力）
		1. 对原始的数据进行分割（Split），得到N个不同的数据分块
		2. 每一个数据分块都启动一个Map进行处理。采用桶排序的方法，每个Map中按照首字母将字符串分配到26个不同的桶中
		3. 按照首字母将Map中不同桶中的字符串集合放置到相应的Reduce中进行处理。具体来说就是首字母为a的字符串全部放在Reduce1中处理，首字母为b的字符串全部放在Reduce2，以此类推

分布式结构化数据表Bigtable
- 设计动机与目标
	- 设计动机：
		- 需要存储的数据种类繁多：Google目前向公众开放的服务很多，需要处理的数据类型也非常多。包括URL、网页内容、用户的个性化设置在内的数据都是Google需要经常处理的
		- 海量的服务请求：Google运行着目前世界上最繁忙的系统，它每时每刻处理的客户服务请求数量是普通的系统根本无法承受的
		- 商用数据库无法满足Google的需求：一方面现有商用数据库设计着眼点在于通用性，根本无法满足Google的苛刻服务要求；另一方面对于底层系统的完全掌控会给后期的系统维护、升级带来极大的便利
	- 基本目标：
		- 广泛的适用性：Bigtable是为了满足系列Google产品而非特定产品存储要求
		- 很强的扩展性：根据需要随时可以加入或撤销服务器
		- 高可用性：Bigtable设计的重要目标之一就是确保几乎所有的情况下系统都可用
		- 简单性：底层系统简单性既可减少系统出错概率，也为上层应用开发带来便利
- 数据模型
	- Bigtable是一个分布式多维映射表，表中的数据通过一个行关键字（Row Key）、一个列关键字（Column Key）以及一个时间戳（Time Stamp）进行索引
		- 行
			- Bigtable的行关键字可以是任意的字符串，但是大小不能超过64KB。Bigtable和传统的关系型数据库有很大不同，它不支持一般意义上的事务，但能保证对于行的读写操作具有原子性（Atomic）
			- 表中数据都是根据行关键字进行排序的，排序使用的是词典序。
			- 一个典型实例，其中com.cnn.www就是一个行关键字。不直接存储网页地址而将其倒排是Bigtable的一个巧妙设计。这样做至少会带来以下两个好处：
				- 同一地址域的网页会被存储在表中的连续位置，有利于用户查找和分析
				- 倒排便于数据压缩，可以大幅提高压缩率
		- 列
			- Bigtable并不是简单地存储所有的列关键字，而是将其组织成所谓的列族（Column Family），每个族中的数据都属于同一个类型，并且同族的数据会被压缩在一起保存。引入了列族的概念之后，列关键字就采用下述的语法规则来定义：
			- 族名：限定词（family：qualifier）
				- 族名必须有意义，限定词则可以任意选定
				- 图中，内容（Contents）、锚点（Anchor）都是不同的族。而cnnsi.com和my.look.ca则是锚点族中不同的限定词
				- 族同时也是Bigtable中访问控制（Access Control）基本单元，也就是说访问权限的设置是在族这一级别上进行的
		- 时间戳
			- Google的很多服务比如网页检索和用户的个性化设置等都需要保存不同时间的数据，这些不同的数据版本必须通过时间戳来区分。图2中内容列的t3、t5和t6表明其中保存了在t3、t5和t6这三个时间获取的网页。Bigtable中的时间戳是64位整型数，具体的赋值方式可以采取系统默认的方式，也可以用户自行定义
			- 为了简化不同版本的数据管理，Bigtable目前提供了两种设置：一种是保留最近的N个不同版本，图中数据模型采取的就是这种方法，它保存最新的三个版本数据。另一种就是保留限定时间内的所有不同版本，比如可以保存最近10天的所有不同版本数据。失效的版本将会由Bigtable的垃圾回收机制自动处理
	- Bigtable对存储在其中的数据不做任何解析，一律看做字符串
	- Bigtable的存储逻辑可以表示为：(row:string, column:string, time:int64)→string
- 系统架构
	- 在Bigtable中Chubby主要有以下几个作用：
		- 选取并保证同一时间内只有一个主服务器（Master Server）
		- 获取子表的位置信息
		- 保存Bigtable的模式信息及访问控制列表
	- 另外在Bigtable的实际执行过程中，Google的MapReduce和Sawzall也被用来改善其性能
	- Bigtable主要由三个部分组成：客户端程序库（Client Library）、一个主服务器（Master Server）和多个子表服务器（Tablet Server）
		- 客户访问Bigtable服务时，首先要利用其库函数执行Open()操作来打开一个锁（实际上就是获取了文件目录），锁打开以后客户端就可以和子表服务器进行通信
		- 和许多具有单个主节点分布式系统一样，客户端主要与子表服务器通信，几乎不和主服务器进行通信，这使得主服务器的负载大大降低
		- 主服务主要进行一些元数据操作以及子表服务器之间负载调度问题，实际数据是存储在子表服务器上
- 主服务器
	- 当一个新子表产生时，主服务器通过一个加载命令将其分配给一个空间足够的子表服务器。创建新表、表合并以及较大子表的分裂都会产生一个或多个新子表。对于前面两种，主服务器会自动检测到，而较大子表的分裂是由子服务发起并完成的，所以主服务器并不能自动检测到，因此在分割完成之后子服务器需要向主服务发出一个通知 
	- 由于系统设计之初就要求能达到良好的扩展性，所以主服务器必须对子表服务器的状态进行监控，以便及时检测到服务器的加入或撤销
		- Bigtable中主服务器对子表服务器的监控是通过Chubby完成的——子表服务器在初始化时都会从Chubby中得到一个独占锁。通过这种方式所有子表服务器基本信息被保存在Chubby中一个称为服务器目录（Server Directory）的特殊目录之中
	- 主服务器会定期向其询问独占锁的状态。如果子表服务器的锁丢失或没有回应，则此时可能有两种情况
		- 要么是Chubby出现了问题（虽然这种概率很小，但的确存在，Google自己也做过相关测试）
		- 要么是子表服务器自身出现了问题。对此主服务器首先自己尝试获取这个独占锁，如果失败说明Chubby服务出现问题，需等待恢复；如果成功则说明Chubby服务良好而子表服务器本身出现了问题
	- 当在状态监测时发现某个子表服务器上负载过重时，主服务器会自动对其进行负载均衡操作
	- 基于系统出现故障是一种常态的设计理念，每个主服务器被设定了一个会话时间的限制。当某个主服务器到时退出后，管理系统就会指定一个新的主服务器，这个主服务器的启动需要经历以下四个步骤：
		- 从Chubby中获取一个独占锁，确保同一时间只有一个主服务器
		- 扫描服务器目录，发现目前活跃的子表服务器
		- 与所有的活跃子表服务器取得联系以便了解所有子表的分配情况
		- 如果元数据表未分配，则首先需要将根子表（Root Tablet）加入未分配的子表中。由于根子表保存了其他所有元数据子表的信息，确保了扫描能够发现所有未分配的子表
- 子表服务器
	- SSTable及子表基本结构
		- SSTable结构：SSTable是Google为Bigtable设计的内部数据存储格式。所有的SSTable文件都存储在GFS上
			- SSTable中数据被划分成一个个的块（Block），每个块的大小是可以设置的，一般为64KB
			- 在SSTable的结尾有一个索引（Index），这个索引保存了块的位置信息，在SSTable打开时这个索引会被加载进内存，用户在查找某个块时首先在内存中查找块的位置信息，然后在硬盘上直接找到这个块
			- 由于每个SSTable一般都不是很大，用户还可以选择将其整体加载进内存，这样查找起来会更快
		- 子表实际组成：每个子表都是由多个SSTable以及日志（Log）文件构成，不同子表的SSTable可以共享
			- Bigtable中的日志文件是一种共享日志，每个子表服务器上仅保存一个日志文件，某个子表日志只是这个共享日志的一个片段。这样会节省大量的空间，但在恢复时却有一定的难度
			- Google为了避免这种情况出现，对日志做了一些改进。Bigtable规定将日志的内容按照键值进行排序，这样不同的子表服务器都可以连续读取日志文件了
			- 一般来说每个子表的大小在100MB到200MB之间。每个子表服务器上保存的子表数量可以从几十到上千不等，通常情况下是100个左右
	- 子表地址结构：子表地址的查询是经常碰到的操作。在Bigtable系统的内部采用的是一种类似B+树的三层查询体系
		- 所有子表地址都被记录在元数据表中，元数据表也是由一个个的元数据子表（Metadata tablet）组成
		- 根子表是元数据表中一个比较特殊的子表，它既是元数据表的第一条记录，也包含了其他元数据子表的地址，同时Chubby中的一个文件也存储了这个根子表的信息。
		- 查询时，首先从Chubby中提取这个根子表的地址，进而读取所需的元数据子表的位置，最后就可以从元数据子表中找到待查询的子表。除了这些子表的元数据之外，元数据表中还保存了其他一些有利于调试和分析的信息，比如事件日志等
	- 为了减少访问开销，提高客户访问效率，Bigtable使用了缓存（Cache）和预取（Prefetch）技术
		- 子表的地址信息被缓存在客户端，客户在寻址时直接根据缓存信息进行查找。一旦出现缓存为空或缓存信息过时的情况，客户端就需要按照图示方式进行网络的来回通信（Network Round-trips）进行寻址，在缓存为空的情况下需要三个网络来回通信。如果缓存的信息是过时的，则需要六个网络来回通信。其中三个用来确定信息是过时的，另外三个获取新的地址 
		- 预取则是在每次访问元数据表时不仅仅读取所需的子表元数据，而是读取多个子表的元数据，这样下次需要时就不用再次访问元数据表
	- 子表数据存储及读/写操作
		- Bigtable将数据存储划分成两块：较新的数据存储在内存中一个称为内存表（Memtable）的有序缓冲里，较早的数据则以SSTable格式保存在GFS中
		- 写操作（Write Op）：先查询Chubby中保存的访问控制列表确定用户具相应写权限，通过认证之后写入的数据首先被保存在提交日志（Commit Log）中。提交日志中以重做记录（Redo Record）的形式保存着最近的一系列数据更改，这些重做记录在子表进行恢复时可以向系统提供已完成的更改信息。
		- 读操作（Read Op）：先通过认证，之后读操作就要结合内存表和SSTable文件来进行，因为内存表和SSTable中都保存了数据

Bigtable 的系统架构主要包括以下几个组件
1. 客户端程序库（Client Library）：
    - 访问接口：提供客户端与Bigtable交互的API，包括数据的读写操作、表的创建和删除等。
    - 缓存和预取：使用缓存和预取技术减少访问开销，提高客户访问效率。子表的地址信息被缓存在客户端，必要时通过网络通信获取新的地址信息。
2. 主服务器（Master Server）：
    - 元数据管理：负责元数据操作，如表的创建、删除、修改等。
    - 负载均衡：管理子表服务器之间的负载调度，确保系统的高可用性和性能。
    - 故障检测：通过Chubby监控子表服务器的状态，及时检测到服务器的加入或撤销。
    - 主服务器选择：通过Chubby确保同一时间内只有一个主服务器。
3. 子表服务器（Tablet Server）：
    - 数据处理：负责处理数据的读写操作，实际数据存储在子表服务器上。
    - 子表管理：管理子表的分裂、合并和分配。
    - 故障处理：负责子表的故障处理和恢复。
4. Chubby：
    - 分布式锁服务：提供分布式锁服务，确保系统的高可用性和一致性。
    - 元数据存储：保存Bigtable的模式信息及访问控制列表。
    - 主服务器选择：通过Chubby确保同一时间内只有一个主服务器。
    - 子表位置信息：保存子表的位置信息，帮助客户端快速定位子表。
5. GFS（Google File System）：
    - 数据存储：提供分布式文件系统，用于存储Bigtable的数据。
    - SSTable：Bigtable的数据存储格式，数据被划分为多个块（Block），每个块的大小可以设置，一般为64KB。SSTable的结尾有一个索引，保存了块的位置信息，索引在SSTable打开时被加载进内存，加速数据查找。
6. 日志（Log）：
    - 提交日志（Commit Log）：记录最近的数据更改，用于子表的恢复。
    - 共享日志：多个子表服务器可以共享日志，日志内容按键值排序，便于不同子表服务器连续读取。


# Hadoop

Hadoop：Apache开源组织的一个分布式计算框架，可以在大量廉价的硬件设备组成的集群上运行应用程序，为应用程序提供了一组稳定可靠的接口，旨在构建一个具有高可靠性和良好扩展性的分布式系统
- 来源：
	- 开源项目Lucene：Java开发的开源高性能全文检索工具包
	- 开源项目Nutch：第一个开源的Web搜索引擎
- 组成：Hadoop生态系统
	- HDFS
	- MapReduce
	- HBase
	- Hadoop Common：Hadoop体系最底层的一个模块，为Hadoop各子项目提供各种工具，如：配置文件和日志操作等。
	- Avro：Avro是doug cutting主持的RPC项目，有点类似Google的protobuf和Facebook的thrift。avro用来做以后hadoop的RPC，使hadoop的RPC模块通信速度更快、数据结构更紧凑。
	- Chukwa：Chukwa是基于Hadoop的大集群监控系统，由yahoo贡献。
	- Hive：hive是基于hadoop分布式计算平台上的提供data warehouse的sql功能的一套软件。使得存储在hadoop里面的海量数据的汇总，即席查询简单化。hive提供了一套QL的查询语言，以sql为基础，使用起来很方便。
	- Pig：对应 Google Sawzall，Pig是SQL-like语言，是在MapReduce上构建的一种高级查询语言，把一些运算编译进MapReduce模型的Map和Reduce中，并且用户可以定义自己的功能。
	- ZooKeeper：对应 Google Chubby，它是一个针对大型分布式系统的可靠协调系统，提供的功能包括：配置维护、名字服务、分布式同步、组服务等。ZooKeeper的目标就是封装好复杂易出错的关键服务，将简单易用的接口和性能高效、功能稳定的系统提供给用户。
- 优点：可扩展、经济、可靠、高效

HDFS：分布式文件系统，对应 Google GFS
- 设计前提与目标：
	- 硬件错误是常态而不是异常
	- 流式数据访问
	- 超大规模数据集
	- 简单一致性模型
	- 移动计算比移动数据更简单
	- 异构软硬件平台间的可移植性
- 体系结构：主从结构体系
	- NameNode：主控制服务器，负责维护文件系统的命名空间（Namespace）并协调客户端对文件的访问，记录命名空间内的任何改动或命名空间本身的属性改动 
	- DataNode：负责它们所在的物理节点上的存储管理
- 保障可靠性的措施：
	- 冗余备份：每个文件存储成一系列数据块（Block），默认块大小为64MB（可配置）。为了容错，文件的所有数据块都会有副本（副本数量即复制因子，可配置）
	- 副本存放：采用机架感知（Rack-aware）的策略来改进数据的可靠性、可用性和网络带宽的利用率
	- 心跳检测：NameNode周期性地从集群中的每个DataNode接受心跳包和块报告，收到心跳包说明该DataNode工作正常
	- 安全模式：系统启动时，NameNode会进入一个安全模式。此时不会出现数据块的写操作
	- 数据完整性检测：HDFS客户端软件实现了对HDFS文件内容的校验和（Checksum）检查
	- 空间回收：文件被用户或应用程序删除时，先把它移动到/trash目录里；只要还在这个目录里，文件就可以被迅速恢复
	- 元数据磁盘失效：NameNode可以配置为支持维护映像文件和事务日志的多个副本，任何对映像文件或事务日志的修改，都将同步到它们的副本上
	- 快照：快照支持存储某个时间的数据复制，当HDFS数据损坏时，可以回滚到过去一个已知正确的时间点。HDFS目前还不支持快照功能
- 提升性能的措施：
	- 副本选择：HDFS会尽量使用离程序最近的副本来满足用户请求，这样可以减少总带宽消耗和读延时
	- 负载均衡：HDFS的架构支持数据均衡策略
	- 客户端缓存：HDFS客户端先把数据缓存到本地的一个临时文件，程序的写操作透明地重定向到这个临时文件
	- 流水线复制：DataNode从前一个节点接收数据的同时，即时把数据传给后面的节点，这就是流水线复制
- 访问接口
	- Hadoop API
		`org.apache.hadoop.conf`
		`org.apache.hadoop.dfs`
		`org.apache.hadoop.fs`
		`org.apache.hadoop.io`
		`org.apache.hadoop.ipc`
		`org.apache.hadoop.mapred`
		`org.apache.hadoop.metrics`
		`org.apache.hadoop.record`
		`org.apache.hadoop.tools`
		`org.apache.hadoop.util`
- 浏览器接口：典型HDFS安装会配置一个Web服务器开放自己的命名空间，其TCP端口可配；默认配置下`http://namenode-name:50070`这个页面列出了集群里的所有DataNode和集群的基本状态

MapReduce：分布式数据处理，对应 Google MapReduce，实现了MapReduce编程框架
- 任务基本要求：待处理的数据集可以分解成许多小的数据集，而且每一个小数据集都可以完全并行地进行处理
- 逻辑模型：
	- 映射阶段：用户输入的数据分割为M个片断，对应M个Map任务。每一个Map操作的输入是数据片断中的键值对<K1,V1>集合，Map操作调用用户定义的Map函数，输出一个中间态的键值对<K2,V2> 集合。接着，按照中间态的K2将输出的数据集进行排序，并生成一个新的<K2,list(V2)>元组，按照K2的范围将这些元组分割为R个片断
	- 化简阶段：每一个Reduce操作的输入是一个<K2,list(V2)>片断，Reduce操作调用用户定义的Reduce函数，生成用户需要的键值对<K3,V3>进行输出
- 实现机制
	1. 分布式并行计算
	2. 本地计算
	3. 任务粒度
	4. Combine（连接）
	5. Partition（分区）
	6. 读取中间结果
	7. 任务管道

HBase：分布式结构化数据表，对应 Google Bigtable，基于Hadoop Distributed File System，是一个开源的，基于列存储模型的分布式数据库。
- HBase是一个分布式的、多版本的、面向列的开源数据库
	- 利用Hadoop HDFS作为其文件存储系统，提供高可靠性、高性能、列存储、可伸缩、实时读写的数据库系统
	- 利用Hadoop MapReduce来处理HBase中的海量数据
	- 利用Zookeeper作为协同服务
- 表的特点：
	- 大：一个表可以有上亿行，上百万列（列多时，插入变慢）
	- 面向列：面向列(族)的存储和权限控制，列(族)独立检索。
	- 稀疏：对于为空(null)的列，并不占用存储空间，因此，表可以设计的非常稀疏。
	- 每个cell中的数据可以有多个版本，默认情况下版本号自动分配，是单元格插入时的时间戳；
	- HBase中的数据都是字符串，没有类型；
- HBase 特点：
	- 强一致性：同一行数据的读写只在同一台Region Server上进行
	- 水平伸缩
		- Region的自动分裂以及Master的balance；
		- 只用增加Datanode机器即可增加容量；
		- 只用增加Region Server机器即可增加读写吞吐量
	- 行事务：同一行的列的写入是原子的
	- Column Oriented + 三维有序
		- SortedMap(RowKey，List(SortedMap(Column，List(Value，Timestamp))))
		- rowKey (ASC) + columnLabel(ASC) + Version (DESC)  --> value
	- 支持有限查询方式和一级索引
		- 仅支持单行事务
		- 仅支持三种查询方式（single row key、range row key、scan  all rows of  table）【可通过hive等实现多表关联查询】
		- 仅基于row key的索引
	- 高性能随机写：WAL (Write Ahead Log)
	- 和Hadoop无缝集成
		- Hadoop分析后的结果可直接写入HBase；
		- 存放在HBase的数据可直接通过Hadoop来进行分析。
- 体系结构：
	- Client
		- 包含访问HBase的接口并维护cache来加快对HBase的访问
	- Zookeeper
		- 保证任何时候，集群中只有一个master
		- 存贮所有Region的寻址入口。
		- 实时监控Region server的上线和下线信息。并实时通知给Master
		- 存储HBase的schema和table元数据
	- Master
		- 为Region server分配region
		- 负责Region server的负载均衡
		- 发现失效的Region server并重新分配其上的region
		- 管理用户对table的增删改查操作
	- Region Server
		- Region server维护region，处理对这些region的IO请求
		- Region server负责切分在运行过程中变得过大的region
	- -ROOT-与.META.表
		- -ROOT-
			- 表包含.META.表所在的区域列表，该表只会有一个HRegion；
			- Zookeeper中记录了-ROOT-表的location
		- .META.
			- 表包含所有的用户空间区域列表，以及RegionServer的服务器地址；
	- Region定位
- 数据模型：
	- 逻辑视图/模型：HBase以表的形式存储数据。表由行和列组成。列划分为若干个列族(row family)
		- 表格里存储一系列的数据行，每行包含一个可排序的行关键字、一个可选的时间戳及一些可能有数据的列（稀疏）
		- 数据行有三种基本类型的定义：行关键字是数据行在表中唯一标识，时间戳是每次数据操作对应关联的时间戳，列定义为：`<family>:<label>（<列族>:<标签>）`
		- HBase 数据表中一些关键概念
			- Row key 键
				- 表中行的键是字节数组(最大长度是 64KB )
				- 任何字符串都可以作为键；
				- 表中的行根据行的键值进行排序，数据按照Row key的字节序(byte order)排序存储；
					- 字典序对int排序的结果是1,10,100,11,12,13,14,15,16,17,18,19,2,20,21,…,9,91,92,93,94,95,96,97,98,99。要保持整形的自然序，行键必须用0作左填充
				- 所有对表的访问都要通过键
					- 通过单个row key访问
					- 通过row key的range
					- 全表扫描
			- Column Family 列族
				- HBase表中的每个列都归属于某个列族，列族必须作为表模式(schema)定义的一部分预先给出。如 create ‘test’, ‘course’
				- 列名以列族作为前缀，每个“列族”都可以有多个列成员(column)；如course:math, course:english
				- 新的列族成员可以随后按需、动态加入
				- 权限控制、存储以及调优都是在列族层面进行的
					- 同一列族成员最好有相同的访问模式和大小特征
				- HBase把同一列族里面的数据存储在同一目录下，由几个文件保存
			- Cell qualifier 单元格修饰符
				- 通过列族:单元格修饰符，可以具体到某个列
				- 可以把单元格修饰符认为是实际的列名
				- 在列族存在，客户端随时可以把列添加到列族
			- Timestamp 时间戳
				- 在HBase每个cell存储单元对同一份数据有多个版本，根据唯一的时间戳来区分每个版本之间的差异，不同版本的数据按照时间倒序排序，最新的数据版本排在最前面
				- 时间戳的类型是 64位整型
				- 时间戳可以由HBase(在数据写入时自动)赋值，此时时间戳是精确到毫秒的当前系统时间
				- 时间戳也可以由客户显式赋值，如果应用程序要避免数据版本冲突，就必须自己生成具有唯一性的时间戳
			- Region 区域
				- HBase自动把表水平划分成多个区域(region)，每个region会保存一个表里面某段连续的数据
				- 每个表一开始只有一个region，随着数据不断插入表，region不断增大，当增大到一个阀值的时候，region就会等分会两个新的region
				- 当table中的行不断增多，就会有越来越多的region。这样一张完整的表被保存在多个Region 上
			- Cell单元格
				- •由行和列的坐标交叉决定
				- 单元格是有版本的
				- 单元格的内容是未解析的字节数组
				- 由`{row key， column( =<family> + <label>)， version} `唯一确定的单元。cell中的数据是没有类型的，全部是字节码形式存贮
			- 锁：HBase的写操作是锁行的，每一行都是一个原子元素，无论对行进行访问的事务设计多少列，对行的更新都是原子的。都可以加锁。这使得加锁模型简单化
	- 物理视图/模型：物理模型实际上就是把概念模型中的一个行进行分割，并按照列族存储，HBase每个列族存储为一个Store
		- 查询时间戳为t7的“contents:”将返回空值，查询时间戳为t8，“anchor:”值为“look.ca”的项也返回空值（空的单元格不存储）
		- 查询“contents:”而不指明时间戳，将返回t5时刻的数据；查询“anchor:”的“look.ca”而不指明时间戳，将返回t7时刻的数据 （未指明时间戳，则返回指定列的最新数据值 ）
		- HBase 中数据表的物理存储方式
			- Table中的所有行都按照row key的字典序排列
			- Table 在行的方向上分割为多个HRegion
			- Region按大小分割的，每个表一开始只有一个region，随着数据不断插入表，region不断增大，当增大到一个阀值的时候，HRegion就会等分会两个新的HRegion。当able中的行不断增多，就会有越来越多的HRegion
			- HRegion是HBase中分布式存储和负载均衡的最小单元。最小单元就表示不同的HRegion可以分布在不同的HRegion server上。但一个HRegion是不会拆分到多个server上的
			- HRegion虽然是分布式存储的最小单元，但并不是存储的最小单元。事实上，HRegion由一个或者多个Store组成，每个store保存一个columns family。
				- 每个Strore又由一个memStore和0至多个StoreFile组成。如图：StoreFile以HFile格式保存在HDFS上
			- HFile的格式：
				- 分为六个部分：
					- Data Block 段：保存表中的数据，这部分可以被压缩
					- Meta Block段 (可选的)：保存用户自定义的kv对，可以被压缩。
					- File Info 段：HFile的元信息，不被压缩，用户也可以在这一部分添加自己的元信息。
					- Data Block Index 段：Data Block的索引。每条索引的key是被索引的        block的第一条记录的key。
					- Meta Block Index段 (可选的)：Meta Block的索引。
					- Trailer段：这一段是定长的。保存了每一段的偏移量，读取一个HFile时，会首先读取Trailer，Trailer保存了每个段的起始位置(段的Magic Number用来 做安全check)，然后，DataBlock Index会被读取到内存中，这样，当检索  某个key时，不需要扫描整个HFile，而只需从内存中找到key所在的block，通过一次磁盘io将整个 block读取到内存中，再找到需要的key
				- Data Block Index采用LRU机制淘汰。
				- HFile的Data Block，Meta Block通常采用压缩方式存储，压缩之后可以大大减少网络IO和磁盘IO，随之而来的开销当然是需要花费cpu进行压缩和解压缩。
				- 目标Hfile的压缩支持两种方式：Gzip，Lzo。
			- HLog(WAL log)：HLog文件就是一个普通的Hadoop Sequence File，Sequence File 的Key是HLogKey对象，HLogKey中记录了写入数据的归属信息，除了table和region名字外，同时还包括 sequence number和timestamp，timestamp是”写入时间”，sequence number的起始值为0，或者是最近一次存入文件系统中sequence number。HLog Sequece File的Value是HBase的KeyValue对象，即对应HFile中的KeyValue，可参见上文描述。
		- 子表服务器
			- 客户端进行更新操作时，首先连接相关的子表服务器，之后向子表提交变更。提交的数据被添加到子表的HMemcache和子表服务器的HLog
			- 提供服务时，子表首先查询缓存HMemcache。若没有，再查找磁盘上的HStore
			- HRegion.flushcache()定期被调用，把HMemcache中的内容写到磁盘上HStore文件里
				- 调用flushcache()方法越少，工作量就越少，而HMemcache就要占用更多的内存空间，启动时HLog文件也需要更多的时间来恢复数据。而调用flushcache()越频繁，HMemcache占用内存就越少，HLog文件恢复数据时也就越快
			- 调用HStore.compact()方法来实现多个HStoreFile合并成一个HStoreFile
			- HRegion.closeAndMerge()可把两个子表合并成一个；HRegion. closeAndSplit()，可将子表分割为两个新子表
		- 主服务器
			- 主服务器维护子表服务器在任何时刻的活跃标记
			- 与Google的Bigtable不同，Bigtable使用分布式锁服务Chubby保证了子表服务器访问子表操作的原子性；HBase不具备这样的Chubby
			- 每个子表都由它所属的表格名字、首关键字和region Id来标识
		- 元数据表：子表的元数据存储在另一个子表里，子表的唯一标识符可以作为子表的行标签，映射子表标识符到物理子表服务器位置的表格称为元数据表
			- 启动时，主服务器立即扫描唯一根子表（其名字是硬编码的）
			- 主服务器扫描元数据子表
			- 主服务器扫描完元数据子表，然后就可以把这些子表分配到子表服务器上去
- HBase 交互接口：HBase 提供的接口
	- •Native Java API，最常规和高效的访问方式，适合Hadoop MapReduce Job并行批处理HBase表数据
	- HBase Shell，HBase的命令行工具，最简单的接口，适合HBase管理使用
	- Thrift Gateway，利用Thrift序列化技术，支持C++，PHP，Python等多种语言，适合其他异构系统在线访问HBase表数据
	- REST Gateway，支持REST 风格的Http API访问HBase， 解除了语言限制
	- Hive，使用类似SQL语言来访问HBase
	- Pig，可以使用Pig Latin流式编程语言来操作HBase中的数据，和Hive类似，本质最终也是编译成MapReduce Job来处理HBase表数据，适合做数据统计
- HBase 优化：
	- 配置优化1
		- `zookeeper.session.timeout(df:180000ms)`
		- `hbase.regionserver.handler.count(df:10)`
		- `hbase.hregion.max.filesize(df:256M)`
		- `hbase.regionserver.global.memstore.upperLimit/lowerLimit(0.4/0.35)`
		- `hfile.block.cache.size(df:0.2)`
		- `hbase.hstore.blockingStoreFiles(df:7)`
		- `hbase.hregion.memstore.block.multiplier(df:2)`
	- 配置优化2：写速度关键因素
		- Table region分布均衡；
		- 单台region server的region数；
		- `hbase.regionserver.handler.count`
		- `hbase.regionserver.global.memstore.upperLimit`
		- `hbase.hregion.memstore.block.multiplier`
		- `hbase.hstore.blockingStoreFiles`
		- `hbase.hregion.max.filesize`
	- 配置优化3：读速度关键因素
		- 单台Region Server上的Region数；
		- StoreFile数；
		- bloomfilter；
		- in-memory flag;
		- blockcache设置；
		- hfile.block.cache.size
	- 客户端优化（native java）
		- 客户端写
			- 多HTable并发写
			- 设置Htable的Auto Flush(false)、Write Buffer(bigger)、WAL Flag(close)
			- 批量写
			- 多线程并发写
		- 客户端读
			- 多HTable并发读
			- 设置Htable的Scanner Caching、Scan Attribute Selection、Close ResultScanner
			- 批量读
			- 多线程并发读
	- 业务优化：在可能的情况下，适当调整业务规则，顺应HBase的特征，以便充分利用HBase的优势
	- 设计优化：
		- 充分利用硬件资源， Pre-Creating Regions
		- 合理设计Row Key
		- 少Column Family（低于3个为宜）
		- In Memory
		- Max Version
		- Time To Live
		- 启用LZO压缩
	- 其他优化：JVM设置避免CMS concurrent mode failure
- HBase 用途
	- HBase适用场景
	- 性能测试
		- HBase基本性能参数

# Amazon云计算中的数据库技术

Amazon平台基础存储架构：Dynamo
- Amazon提供的云计算服务
	- 弹性计算云EC2
	- 简单存储服务S3
	- 简单数据库服务Simple DB
	- 简单队列服务SQS
	- 弹性MapReduce服务
	- 内容推送服务CloudFront
	- 电子商务服务DevPay
	- 灵活支付服务FPS
- Dynamo在Amazon服务平台的地位：面向服务的Amazon平台架构
	- Amazon平台的架构是完全的分布式、去中心化
	- 需求——Amazon平台中有很多服务对存储的需求只是读取、写入，（满足简单的键/值式存储）
- Dynamo：Amazon平台基础存储架构
	- 简单的键/值方式存储数据，不支持复杂的查询
	- 存储的是数据值的原始形式（bit），不解析数据的具体内容、不识别任何数据结构，这使得它几乎可以处理所有的数据类型
- Dynamo架构的主要技术
	- 数据均衡分布：改进的一致性哈希算法，数据备份
		- 一致性哈希算法：平衡性、单调性、分散性、负载
		- 两步进行：
			- 求出设备节点的哈希值，并配置到环上的一个点；
			- 接着计算数据的哈希值，按顺时针方向将其映射到环上距其最近的节点；
			- 添加新节点时，按照上述规则，调整相关数据到新的节点上。删除节点和添加节点过程相反
		- Dynamo采用的改进算法
			- 虚拟节点
			- 数据分区和等份存储
		- 数据备份：当数据被均匀存储到环上各节点后，Dynamo将冗余存储数据（备份数据）
		- 思考：Amazon可以保证相邻的节点分别位于不同地区区域，即使某个数据中心由于自然灾害或断电的原因整体瘫痪，仍可以保证在世界上其他数据中心中保存有数据的备份。这里就有一个非常重要的问题——如何进行节点分布，保证相邻节点位于不同的数据中心 ？
	- 数据冲突处理：向量时钟（vector clock）
		- Dynamo系统选择牺牲一致性来换取系统的可靠性和可用性
		- 解决数据冲突：
			- 最终一致性模型（Eventual Consistency）
			- 向量时钟
	- 容错机制：
		- 临时故障处理：Hinted handoff（数据回传机制），参数（W,R,N）可调的弱quorum机制
			- 在数据读写中采用了一种称为弱quorum（Sloppy quorum）的机制，涉及三个参数W、R、N
				- W—代表一次成功的写操作至少需要写入的副本数
				- R—代表一次成功读操作需由服务器返回给用户的最小副本数
				- N—每个数据存储的副本数
			- 满足R+W>N，用户即可自行配置R和W
			- 优势：实现可用性与容错性之间的平衡
		- 永久故障后的恢复：Merkle哈希树
			- 每个虚拟节点保存三颗Merkle树，即每个键值区间建立一个Merkle树
			- 哈希树的叶子节点是存储每个数据分区内所有数据对应的哈希值，父节点是其所有子节点的哈希值
				- 哈希函数：一种将输入的字节数据散列化的不可逆的函数，例如SHA256相当于一个消息的缩写。
					- 相同的数据输入将得到相同的结果
					- 输入的数据一旦稍有变化，就会得到一个千差万别的结果，结果无法预测
					- 正向计算(由数据计算其对应的Hash值)十分容易
					- 逆向计算(俗称“破解”由Hash值计算其对应的数据)相当困难
	- 成员资格以及错误检测：基于gossip的成员资格协议和错误检测
		- Gossip（闲聊）协议
			- 基于Gossip协议的成员资格检测机制
			- 基于Gossip协议的最优传遍路
		- 错误检测机制

简单存储服务S3
- 概念：
	- S3系统构架在Dynamo之上，采取的并不是传统的关系数据库存储方式，原因为
		- 使文件操作尽量简单、高效
		- 使用关系数据库只会增加系统的复杂性
	- 对象
		- 数据（任意类型）和元数据（描述数据的数据 ）
		- 元数据是通过一对键-值（Name-Value）集合来定义，系统默认元数据有：
			- last-modified：对象被最后修改的时间
			- ETag：利用MD5哈希算法得出的对象值
			- Content-Type：对象的MIME（多功能网际邮件扩充协议）类型，默认为二进制/八位组
			- Content-Length：对象数据长度，以字节为单位
	- 键：对象的唯一标示符
	- 桶
		- 存储对象容器（最多创建100个桶，不限桶中数量）
		- 桶具体命名规则
		- 建议使用规则
- 基本操作：根据Amazon提供的技术文档，目前S3支持的主要操作包括：Get、Put、List、Delete和Head
	- 操作目标：桶、对象
- 数据一致性模型：S3系统采用冗余存储
	- 优势：某些服务器出现故障时用户仍然可以对其数据进行操作
	- 弊端：用户在操作时可能会出现如下几种情况
		- 一个进程写入一个新的对象并立即尝试读取它，但在该改变被传送到S3的多个服务器前，服务器对该操作可能返回“键不存在”
		- 一个进程写入一个新的对象并立即尝试列出桶中已有的对象，但在该改变被传送到S3的多个服务器前，该对象很可能不会出现在列表中
		- 一个进程用新数据替换现有的对象并立即尝试读取它，但在该改变被传送到S3的多个服务器前，S3可能会返回以前的数据
		- 一个进程删除现有的对象并立即尝试读取它，但在该改变被传送到S3的多个服务器前，S3可能会返回被删除的数据
		- 一个进程删除现有的对象并立即尝试列出桶中的所有对象，但在该改变被传送到S3的多个服务器前，S3可能会列出被删除的对象
	- 出现这些现象是因为S3为了保证用户数据的一致性而采取的一种折中手段，即在数据被充分传播到所有的存放节点之前返回给用户的仍是原数据

简单队列服务SQS (Simple Queue Service)：Amazon为解决其云计算平台之间不同组件的通信而专门设计开发
- SQS由三个基本部分组成
	- 系统组件（Component）
	- 队列（Queue）
	- 消息（Message）
- 两个重要概念
	- 消息是发送者创建的具有一定格式的文本数据，接收对象可以是一个或多个组件。消息的大小是有限制的，目前Amazon规定每条消息不得超过8KB，但是消息的数量并未做限制
	- 队列是存放消息的容器，类似于S3中的桶，队列的数目也是任意的，创建队列时用户必须给其指定一个在SQS账户内唯一的名称
- 消息
	- 消息的格式
		- 消息ID（Message ID）
		- 接收句柄（Receipt Handle）
		- 消息体（Body）
		- 消息体MD5摘要（MD5 of Body）
	- 消息取样
		- 队列中的消息冗余存储，目的是为了保证系统的高可用性
		- 基于加权随机分布（Based on a Weighted Random Distribution）的消息取样

简单数据库服务 Simple DB (SDB)
- 主要用于存储结构化的数据，并为这些数据提供查找、删除等基本的数据库功能
- SDB基本结构图中包含了SDB中以下几个最重要的概念
	- 用户账户（Customer Accout）
	- 域（Domain）：数据容器
	- 条目（Item）：一个实际的对象
	- 属性（Attribute）：条目的特征
	- 值（Value）：每个条目的某属性的具体内容
- SDB和关系型数据库有很多相同之处，但也有很大的不同
	- 传统的关系数据库——表结构
	- SDB树状结构
	- SDB为了系统的高可用性采取了最终一致性数据模型，每次操作设定了一个超时值，同时SDB也对关系数据库做了一些有益的改进
- Simple DB和其他AWS的结合使用

关系数据库服务RDS
- SQL和NoSQL数据库的对比
- RDS数据库原理：
	- Amazon RDS—一种云中的MySQL数据库系统，采用集群方式将MySQL数据库移植到云中，在一定的范围内解决了关系数据库的可扩展性问题
	- MySQL集群采用了Share-Nothing架构
	- 集群MySQL通过表单划分（Sharding）的方式将一张大表划分为若干个小表，分别存储在不同的数据库服务器上，从逻辑上保证了数据库可扩展性
	- 集群MySQL通过主从备份和读副本技术提高可靠性和数据处理能力
- RDS的使用：
	- Amazon将RDS中的MySQL服务器实例称做DB Instance，通过基于Web的API进行创建和管理，其余的操作可以通过标准的MySQL通信协议完成
	- 可以通过两种工具对RDS进行操作
		- 命令行工具：Amazon提供的Java应用套装，负责处理DB Instance的管理，比如创建、参数调整、删除等
		- MySQL客户端：可以与MySQL服务器进行通信的应用程序，比如MySQL Administrator客户端

# 微软云计算中的数据库技术

微软云计算平台
- 微软云计算技术有效结合了两种方式优点
	- 云计算平台提供了可以通过互联网访问的基础设施
	- 开发运行在本地的应用程序时，用户也可以在云中存储数据或依赖其他的云计算基础设施服务
- Windows Azure属于PaaS模式，平台包括一个云计算操作系统和一系列为开发者提供的服务
	- Windows Azure 云计算平台最底层，微软云计算操作系统，提供了一个在微软数据中心服务器上运行应用程序和存储数据的Windows环境
	- SQL Azure 云中关系数据库，为云中基于SQL Server的关系型数据提供服务  
	- Windows Azure AppFabric 为在云中或本地系统中的应用提供基于云的基础架构服务。部署和管理云基础架构的工作均由AppFabric完成，开发者只需要关心应用逻辑。  
	- Windows Azure Marketplace 为购买云计算环境下的数据和应用提供在线服务

微软云操作系统Windows Azure：Windows Azure提供托管、可扩展、按需应用的计算和存储资源，同时还提供云平台管理和动态分配资源控制手段。
- 最新版本包含5部分
	- 计算服务  为在Azure平台中运行的应用提供支持
	- 存储服务  用来存储二进制和结构化的数据
	- Fabric 控制器  部署、管理和监控应用
	- 内容分发网络CDN  通过维持世界各地数据缓存副本，提高全球用户访问Windows Azure存储中的二进制数据的速度
	- Windows Azure Connect  在本地计算机和Windows Azure之间创建IP级连接
- Windows Azure计算服务：
	- Windows Azure能够自动虚拟出虚拟机
		- Windows Azure计算服务可以支持运行有大量并行用户的大型应用程序
		- Windows Azure中，每个虚拟机运行一个64bit的Windows Server 2008，这些虚拟机由微软数据中心负责维护和管理，每个实例都运行在自己的虚拟机上
		- 用户只关心如何构建和配置自己的应用程序
	- Windows Azure应用程序包括Web Role实例、Worker Role实例和VM Role实例
		- Web Role  使基于Web的应用的创建过程变得简单
		- Worker Role  用来运行各种各样的基于Windows的代码
		- VM Role  运行系统提供的Windows Server 2008 R2镜像
	- 支持HTTP、HTTPS和TCP协议
	- 创建Windows Azure应用时，可以任意结合使用Web Role、Worker Role和VM Role实例
- Windows Azure存储服务
	- Windows Azure存储服务数据存储结构
		- Blob数据类型  存储二进制数据，可以存储大型的无结构数据，容量巨大，能够满足海量数据存储需求
		- Table数据类型  能够提供更加结构化的数据存储
		- Queue类型  和微软消息队列（MSMQ）相近，用来支持在Windows Azure应用程序组件之间进行通信

微软云关系数据库SQL Azure
- SQL Azure
	- SQL Azure是微软的云中关系型数据库，是基于SQL Server技术构建的，主要为用户提供数据应用
	- SQL Azure提供了关系型数据库存储服务，包含三部分
		- SQL Azure数据库
		- SQL Azure 报表服务
		- SQL Azure 数据同步
- SQL Azure关键技术
	- SQL Azure 数据库：SQL Azure的一种云服务，提供了核心的SQL Server数据库功能
		- SQL Azure数据库应用能够使用任何现有的SQL Server客户端，包括Entity Framework、ADO.NET、ODBC和PHP等
		- 每个SQL Azure账户都拥有一个或多个逻辑服务器，这些逻辑服务器可以组织账户数据和账单
		- SQL Azure与SQL Server差别：SQL Azure省略了SQL Server中的一些技术点，比如SQL CLR、全文本搜索技术等 ；相比于SQL Server所提供的单个实例而言，SQL Azure运行环境比较稳定，应用获取的服务也比较健壮；SQL Azure数据库存储的所有数据均备份了3份
	- SQL Azure报表服务器
		- SQL Azure Reporting主要有两个使用场景：
			- 第一，SQL Azure报表创建的报表可以发布到某一个门户上，云端用户可以访问这个门户的报表，也可以通过URL地址直接访问报表；
			- 第二，ISV（Independent Software Vendor，独立的软件开发商）能够嵌入发布到SQL Azure报表门户的报表
		- SQL Azure报表服务与存储在SQL Azure数据库中的数据相互作用
		- 注意：SQL Azure Reporting并没有实现本地情况下SSRS提供的所有的功能
	- SQL Azure数据同步
		- 目的：为了提高存储数据的访问性能，同时确保网络发生故障时应用仍然能够访问数据库
		- SQL Azure数据同步技术
			- SQL Azure数据库与SQL Server数据库之间的数据同步
			- SQL Azure数据库之间的同步
		- SQL Azure数据同步服务使用“轮辐式（hub-and-spoke）”模型，所有的变化将会首先被复制到SQL Azure数据库“hub”上，然后再传送到其他“spoke”上
- SQL Azure应用场景
	- Web应用
		- 对于大部分Web站点而言，用户输入和电子商务交易的数据都需要使用数据库进行存储
		- SQL Azure提供了高可用并具有容错性能的数据库服务
	- 部门级应用
		- 在一些大型的组织中，要求数据库服务器具有容错的功能保证服务不中断
		- 解决上述问题最好办法：将控制逻辑移动到Windows Azure，这样数据访问代码和数据本身都存放在同一个数据中心中
	- 数据集应用
		- 实例：保险公司数据集应用
		- SQL Azure时有三个任务（图示）
			- 在SQL Azure中创建一个数据库用来存储产品数据和顾客数据
			- 在数据中心中创建一个Sync Framework提供者
			- 为销售人员创建一个二级的Sync Framework提供者
	- “软件+服务”应用
		- ISVs通常都具有较好的软件开发能力，他们拥有开发基础架构的能力。因而，ISVs可以使用SQL Azure提供“软件+服务”解决方案，这些供应商称为S2（Software and Services）供应商
		- 金融、政府机关、医疗和房地产等行业通常需要存储大量的历史数据，S2供应商可以提供比较好的支撑
		- S2供应商通常结合使用SQL Azure和Windows Azure
- SQL Azure和SQL Server对比
	- 物理管理和逻辑管理
		- SQL Azure在管理上突出强调了物理管理，能够自动复制所有存储数据以提供高可用性，同时还可以管理负载均衡、故障转移等功能 
		- 用户不能管理SQL Azure的物理资源
		- SQL Azure不能使用SQL Server备份机制，所有的数据都是自动复制备份
	- 服务提供
		- 部署本地SQL Server时，需要准备和配置所需要的硬件和软件
		- 用户在Windows Azure平台上创建了账户后，便可以使用SQL Azure数据库，同时还可以访问所有提供的服务
		- 每个SQL Azure订阅都会绑定到微软数据中心的某个SQL Azure服务器上
		- SQL Azure服务器上的数据库通常会在数据中心其他物理机上进行备份
	- Transact-SQL支持：SQL Azure中由微软进行物理资源的管理，因而这些类型的参数并不适用于SQL Azure
	- 特征和类型：SQL Azure不支持SQL Server的所有特征和数据类型。在现今版本的SQL Azure中，不支持分析、复制、报表和服务代理等服务