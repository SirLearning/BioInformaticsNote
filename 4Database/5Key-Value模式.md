[[BioInformaticsNote/4Database/README|README]]

# 互联网发展趋势和对数据管理模式的要求

 **互联网的发展趋势**
- **物物互联**：物联网技术的发展。
- **人人互联**：社交网络的普及。
- **人物互联**：人与物之间的交互。

**数据量的增长**
- **数据度量**：
	- 1 Byte = 8 Bit
	- 1 KB = 1,024 Bytes
	- 1 MB = 1,024 KB
	- 1 GB = 1,024 MB
	- 1 TB = 1,024 GB
	- 1 PB = 1,024 TB
	- 1 EB = 1,024 PB
	- 1 ZB = 1,024 EB
	- 1 YB = 1,024 ZB

 **大数据的特征（4V）**
- **Volume（大量化）**：数据量庞大，如淘宝等电商平台的数据量已达几PB。
- **Variety（多样化）**：数据格式多样，包括结构化数据、半结构化数据和非结构化数据。
- **Velocity（快速化）**：数据需要快速处理和响应，如电商数据的实时分析。
- **Value（价值密度低）**：大量数据中包含的信息价值密度低，需要有效的数据挖掘方法。

大数据类型：
- 结构化数据：二维表（关系型）
	- 先有结构、再有数据
- 半结构化数据：树、图，XML、HTML之类自描述
	- 先有数据，再有结构
- 非结构化数据：无，文档、图片、视频音频

关系数据库的瓶颈
- **传统关系数据库的优势**
	- **成熟**：发展已有40年历史，如Oracle、SQL Server、MySQL等。
	- **结构化数据处理**：擅长处理结构化数据和复杂的查询。
- 互联网计算环境：数据的规模较大，要处理的互联网数据有很多是非结构化的数据，很多互联网应用（例如互联网搜索、电子商务等应用）并不需要对数据进行复杂的查询
	- 高并发读写需求：High performance
		- 问题：数据库读写压力巨大，硬盘IO无法承受。
		- 解决方案：
			- Master-Slave架构，主从分离
			- 分库、分表，缓解写压力，增强读库的可扩展性
	- 海量数据的高效率存储和访问需求：Huge Storage
		- 问题：存储记录数量有限，SQL查询效率极低
		- 解决方案：分库、分表，缓解数据增长压力
	- 高可扩展性和高可用性的需求：High Scalability && High Availability
		- 问题：传统关系数据库难以扩展和保证高可用性。
		- 解决方案：分布式数据库系统，如Sharding、Replication等。
			- 在关系数据库中解决数据库扩展性的思路主要有两个
				- 水平分区（数据分片 sharing）或垂直分区（功能业务分区）：这样虽然可以很好解决数据库的扩展性问题，但是在实际使用中，一旦采用了数据分片或者功能分区，必然导致牺牲“关系型”数据库的最大优势-join，对业务的局限性会很大，而且数据库也退化成为一个简单的存储系统
				- Master-Slave复制方式：通过读写分离在某种程度上解决扩展性的问题，但是这种方案中，由于每个数据库节点必须保存所有的数据，这样每个存储的IO必然会成为扩展的瓶颈，并且master也是一个瓶颈
			- 关系数据库的瓶颈：解决方案的问题
				- 分库分表缺点：
					- 受业务规则影响，需求变动导致分库分表的维护复杂
					- 系统数据访问层代码需要修改
				- Master-Slave缺点:
					 - Slave实时性的保障，对于实时性很高的场合可能需要做一些处理
					 - 高可用性问题，Master就是那个致命点，容易产生单点故障

# NoSQL数据库

NoSQL：
   - 定义：**Not Only SQL**，非关系型数据库
   - BASE 模型：
	   - Basically Available(基本可用)：支持分区失败
	   - Soft-state(软状态)：状态可以有一段时间不同步
	   - Eventually consistent(最终一致)：最终数据是一致的
   - 优势：
	   - 易扩展：NoSQL数据库种类繁多，但是一个共同的特点都是去掉关系数据库的关系型特性。数据之间无关系，这样就非常容易扩展。也无形之间，在架构的层面上带来了可扩展的能力。甚至有多种NoSQL之间的整合。
	   - 灵活的数据模型：NoSQL无需事先为要存储的数据建立字段，随时可以存储自定义的数据格式。而在关系数据库里，增删字段是一件非常麻烦的事情。如果是非常大数据量的表，增加字段简直就是一个噩梦。
	   - 高可用：NoSQL在不太影响性能的情况，就可以方便的实现高可用的架构。比如Cassandra，HBase模型，通过复制模型也能实现高可用。
	   - 大数据量，高性能：NoSQL数据库都具有非常高的读写性能，尤其在大数据量下，同样表现优秀。这得益于它的无关系性，数据库的结构简单。
   - 缺点：
	   - 很难实现数据的完整性
	   - 缺乏强有力的技术支持
	   - 开源数据库从出现到用户接受需要一个漫长的过程
	   - 关系型数据库在设计时更能够体现实际要求
   - 和关系型数据库的区别：
	   - 横向和纵向扩展能力
		   - 关系型数据库通常部署在一台服务器上，通过增加处理器、内存和硬盘来升级。部署在多台服务器上的关系型数据库通常是依赖互相复制来保持数据同步。
		   - NoSQL数据库可以部署在单服务器上．但更多的部署是成云状分布。
	   - 列，key/value存储，数组(Tuples)存储
		   - 关系型数据库通常是由表或视图里的字段构成(固定的结构，用各种操作相互关联)。
		   - NoSQL数据库通常存储的是一对键值或数组(Tuples)，其结构不周定，只是一个有顺序的数据队列。
	   - 数据的内存和硬盘使用
		   - 关系型数据库通常是驻留在一个硬盘内或一个网络存储空间里。查询或存储过程操作会把数据集提取到内存空间里。
		   - 一些(并不是全部)NoSQL数据库可以直接在硬盘上操作，也可以通过内存来加快速度。

分布式数据系统的CAP原理
- 三要素：
	- 一致性（Consistency）是指执行了一次成功的写操作之后，未来的读操作一定可以读到这个写入的值。
	- 可用性（Availability）(指的是快速获取数据)：每一次操作总是能够在确定的时间返回 。
	- 分区容忍性（Partition-tolerance）系统中任意信息的丢失或失败不会影响系统的继续运作。
- CAP原理：在分布式系统中，这三个要素最多只能同时实现两点，不可能三者兼顾。
	- 对大型网站，可用性与分区容忍性优先级要高于数据一致性，一般会尽量朝着 A、P 的方向设计，然后通过其它手段保证对于一致性的商务需求。
	- 当然，牺牲一致性，并不是完全不管数据的一致性，否则数据是混乱的，那么系统可用性再高分布式再好也没有了价值。牺牲一致性，只是不再要求关系型数据库中的强一致性，而是只要系统能达到最终一致性即可。
	- 通常是通过数据的多份异步复制来实现系统的高可用和数据的最终一致性的。

NoSQL数据模型
- NoSQL运动两个核心理论基础：
	- •Google的BigTable：BigTable提出了一种很有趣的数据模型，它将各列数据进行排序存储。数据值按范围分布在多台机器，数据更新操作有严格的一致性保证。
	- Amazon的Dynamo：Dynamo使用的是另外一种分布式模型。Dynamo的模型更简单，它将数据按key进行hash存储。其数据分片模型有比较强的容灾性，因此它实现的是相对松散的弱一致性：最终一致性。
- 分类：
	- **Key/Value存储**：如Tokyo Cabinet/Tyrant、Berkeley DB、MemcacheDB、Redis。
	- **列存储**：如Bigtable、HBase。
	- **文档存储**：如MongoDB、CouchDB。
	- **图存储**：如Neo4j、FlockDB、InfoGrid。
	- 对象存储：如db4o、Versant
	- xml数据库：如Berkeley DB XML、BaseX

# Key/Value 数据存储模式

**Key/Value存储**：通过键（Key）快速查询到其对应的值（Value）。
- 分布式key/value存储系统比关系数据库更适于互联网环境，所以，只需主键简单查询的需求广泛存在于互联网应用中，分布式key/value存储和管理系统日益受到受到重视
- 一个好的 key/value存储系统需要满足以下条件：简单来说，就是数据不能丢失，服务不能中断，能对故障进行感知并能自动恢复，读写性能极高
	- Availability 可用性
	- Scalability 可扩展性
	- Failover 故障恢复
	- Performance 高性能
- 应用：Amazon Dynamo, Yahoo! PNUTS等
	- 也有一些Key/Value的变体，如Google BigTable, Facebook Cassandra, HyperTable等
- 特点：
	- 数据模型：无数据模式，与数据项相关的内容都存储在一个单独的数据项中
		- 要获取一个数据项的相关内容无需多个表之间的Join操作
		- 便于扩展
		- 重复存储
		- 在数据模型设计时，没有范式的概念，没有表示关系和关系约束的机制（增加了应用程序的负担）
	- 数据访问机制:
		- API，而非SQL，少数提供类似SQL的语法定义过滤规则。
		- 关系数据库有存储过程、触发器等，将数据处理逻辑在数据存储和管理系统中实现，但Key/Value的这些处理逻辑全部实现在应用代码中。
	- 应用接口:
		- SOAP/REST服务接口
		- 一个数据项和一个“对象”对应，直接映射到应用程序代码，无需进行对象关系映射.
- 与关系数据库的比较
	- 优点：
		- 便于扩展，适于云计算的环境
		- 与应用程序代码的兼容性更好
	- 缺点：
		- 数据完整性约束转移至应用程序
		- 目前的很多Key/Value数据存储系统之间不兼容
		- 在云环境中，很多用户和应用使用同一个系统。为了避免一个进程使共享环境超载，往往严格限制一个单独的查询所能够产生的全局影响
			- 例如，在Amazon  Simple DB中，不允许用户运行一个超过5秒钟的查询，在Goolge AppEngine数据存储中，用户一次查询返回的数据项只允许在1000条以内。这对于很多商业应用来说，是不现实的。特别是对于数据分析应用，例如用户使用模式跟踪、推荐系统等来说，这样的限制是不可容忍的。

数据结构：域(Domain)＋数据项(Item)
- 域：类似于传统关系数据库中的“表”，但域无结构，作用是容纳数据项
- 数据项：用Key定义
	- 一个域中的不同数据项可能具有不同的结构，数据属性全部是字符串类型，但在有些实现中，属性也可以具有简单的类型，如整型、字符串数组等
	- 一个域中，不同数据项中很可能有重复存储的数据内容，好在由于磁盘的单位价格越来越低，重复存储并不是很大的问题了，而这种数据结构却为系统的可伸缩性带来了很大的便利，数据可以容易得扩展到其他机器上

Key/Value模型中的 API：
- 对应关系数据库中的SQL：
	- 关系数据库的数据创建、更新、删除和获取都使用SQL完成
	- SQL查询可以从单个表或者通过多个表的Join操作来获取数据，SQL查询包括聚集、复杂的数据过滤等功能
	- 传统关系数据库还包括将一些数据处理逻辑嵌入到数据存储中的实现，例如存储过程、触发器等
- Key/Value数据的创建、更新、删除和获取都是用API方法调用
	- 例如：亚马逊的分布式Key/Value数据存储与管理系统Dynamo是通过一个简单的接口将对象与key关联，有两个操作：get()和put()

# Key/Value 相关技术

数据划分：在系统扩展时，系统提供一定的机制将数据切分到新增的机器（或节点）上。在分布式Key/Value数据存储系统中，一般将切分功能以数据自动迁移的方式实现。
- 原因：系统要具备高扩展性，因此，增加和删除机器是频繁的操作，为将数据均匀分散到集群中，需要对数据进行切分
- 在数据存储系统中，有三种数据切分技术：
	- 基于简单哈希算法的Key/Value数据切分机制
		- 基本原理：根据Key和哈希算法来决定数据存储的节点
		- 算法：有N个cache服务器（后面简称cache），将一个数据对象object映射到N个cache上，采用哈希算法： hash(object)%N，得到数据object的hash值，然后将数据对象均匀的映射到到N个cache服务器上。
		- 简单哈希算法优点：简单、快速
		- 简单哈希算法缺点：当节点加入或者退出时，原有的数据将定位到不同的节点上，当节点数多时，数据迁移的代价很高
	- 基于一致性哈希算法的数据切分机制：目前比较公认的解决办法就是一致性哈希(consistent hashing)
		- 一致性哈希算法：哈希函数的输出范围被看作一个固定的“环”。系统中的每个节点被赋予环中的一个随机值，该随机值用来表示其在“环”中的位置。每个“键值”对应一个数据项，根据该键值的哈希值可生成数据项在环中的位置position = hash(key)，然后顺时针沿着环找到value大于position的第一个节点，这个节点就是该数据项的存储节点
		- consistent  hashing算法的基本原理：
			- 环形hash空间
			- 把数据对象映射到hash空间
			- 把cache映射到hash空间
			- 把对象映射到cache
			- 考察cache的变动
				- 移除cache
				- 添加cache
		- 一致性哈希算法优点：每个节点都负责存储环中该节点与其后继节点之间区域对应的存储对象，也称为“区间负责制”。区间负责制使得节点的加入和退出只需要其邻居节点进行数据迁移，而不影响其它节点
		- 一致性哈希算法缺点：采用随机的位置值来决定数据项存储在哪个节点上，这导致节点之间负载不均衡
	- 基于映射表的数据切分机制：
		- 对数据进行水平切分，数据存放在哪个节点上是非常灵活的（并不一定是根据哈希函数来决定）
		- 将数据和存储单元之间的映射关系存放在一个单独的表中，当需要访问某个节点的时候，首先去映射表中查找，找到以后再定位到相应节点

复制和一致性保障：
- 原因：
	- 在分布式Key/Value数据存储系统中，为了获得更好的可用性和持久性，需要将数据复制到多个节点上。
	- 在大规模互联网计算环境下，出于提高系统可用性的考虑，分布式Key/Value数据存储系统在副本数据的一致性保障方面最典型的一个特征是不保证严格的一致性。
	- 最终一致性是大多数分布式Key/Value存储系统的选择，即只保障更新操作最终被传播到各个副本上，而在此之前，各个副本的数据之间可能会出现冲突。
		- 还有一些分布式Key/Value存储系统选择在严格一致性和最终一致性之间的折中方案。
- 复制协议包括两种类型：
	- 主动复制：每个副本有一个关联的进程，该进程执行更新操作。操作被发送到每个副本。
	- 基于法定数量的协议，其基本思想是：在读或写一个复制的数据项之前要求申请并获得多个服务器的允许。
- 策略：
	- 分布式系统中的数据复制和一致性保障机制：为了达到高可用性和数据不丢失，需要通过复制将数据备份到多台机器
		- replication(复制机制)的实现一般是通过Master与replica之间的TCP/IP连接
		- 然后根据相应的一致性策略将数据分发到replica上，这里的一致性策略主要包括两项：
			- replica能够延迟master的时间，就是说，在这个时间内更新的数据，replica可能是看不到的。例如你设置的一致性时间是3s，那么在某个特定的时刻，replica上的数据实际上可能是master 3s 以前的snapshot。
			- master事务提交返回之前是否需要得到replica的确认。为了尽量保证数据不丢失，master需要得到一定数量的replica确认数据更新成功之后才能提交事务。
		- 关于数据可靠性和性能之间，是需要进行折衷的，很显然，越是高的数据保障，那么性能肯定会受到影响。在这样的情况下，需要对上层的应用进行分析，看是否允许丢失一部分数据。
		- 另外，还有一个问题就是，数据的同步是采用master分发还是replica定时请求的问题，两者各有优缺点，前者会在replica较多的情况下遇到瓶颈，而后者可能会有一些延迟。
		- 向量时钟Vector clock 是Dynamo采用的一种数据版本管理方法。
			- 向量时钟Vector clock 机制：
				- Vector clock是一个(node, counter) 值对列表；
				- 每个数据对象的每个版本都有一个vector clock；
				- 如果数据对象第一个VC中的counter小于等于第二个VC中所有节点的counter，那么，第一个的版本比第二个旧，可以舍弃。
				- 一个对象D的Vector clock历史状况
	- Key/Value 一致性模型的实现机制举例-----Dynamo的最终一致性
		- 前提：始终可写“always writable”
			- 数据项的写操作无需等到更新操作传播到所有副本上便可返回给客户，这就会导致随后的读操作可能会读到更新操作之前的数据版本。在没有任何节点失效发生的情况下，更新操作最终会传播到所有副本。
			- Dynamo最终一致性的实现机制：配额设置
				- 三个关键参数(N,R,W)：
					- N指一份数据将被复制到N 个节点上；
					- R表示在读取某一存储的数据时，最少参与节点数，也就是最少需要有多少个节点返回存储的信息才算是成功读取了该数据内容；
					- W表示在存储某一个数据时，最少参与节点数，也就是最少要有多少个节点表示存储成功才算是成功存储了该数据。
				- 配额设置用来灵活地调整系统的可用性与一致性，比如, N=3时
					- 如果R=1，表示最少只需要去一个节点读数据即可，读到即返回，这时是可用性是很高的，但并不能保证数据的一致性；
					- 如果R=1，W=1，那可用性更新是最高的一种情况，但这时完全不能保障数据的一致性，因为在可供复制的N个节点里，只需要写成功一次就返回了，也就意味着，有可能在读的这一次并没有真正读到需要的数据（一致性相当的不好）。􀁺􀁺
					- 如果W=R=3，每次写的时候，都保证所有要复制的点都写成功，读的时候也是都读到，这样子读出来的数据一定是正确的，但是其性能大打折扣，也就是说，数据的一致性非常的高，但系统的可用性却非常低了。
				- 复制中的一致性，采用类似于基于法定数量协议实现。
				- 当配置R+W > N时，就和法定数量的协议完全一样；读(或写)操作的延迟是由R(或W)副本中最慢的副本来决定的。
				- (N,R,W) 的值典型设置为(3, 2 ,2),兼顾性能与可用性。R 和W 直接影响性能、扩展性、一致性。

可用性保障机制
- 节点失效情况下的冲突解决：
	- 在分布式Key/Value数据存储和管理系统中，数据项的写操作无需等到更新操作传播到所有副本上便可返回给客户，这就会导致随后的读操作可能会读到更新操作之前的数据版本。在没有任何节点失效发生的情况下，更新操作最终会传到所有副本上。但是，在有本节点失效的情况下，更新操作则可能无法到达所有副本，这就会导致不一致的产生。
	- 在大多数情况下，新的版本可以覆盖旧的版本，系统自身可以进行版本的同步。但是，在节点失效和并发更新同时发生的情况下，就会出现不同的版本分支。在这种情况下，系统自身无法解决版本冲突，转而由客户端来解决。
	- master管理写请求而replica管理读请求，至于如何决定读写请求的分发，可以使用monitor节点，由它来作为读写的入口， 如下图，然后Monitor管理集群的状态和生命周期，例如Master fail后，monitor将收到事件，它将发起一次选举选出新的Master，一般的选举算法就是在集群中寻找最后一次更新的节点，因为往往它的数据是最新。
	- 还有就是在有新的机器加入集群的情况下，Monitor会告诉新机器集群内的master是谁，replica机器才能与master取得连接同步数据。
- 节点失效情况下的更新传播和同步：传统的法定数量协议并没有考虑节点失败或网络分区的情况，即使在最简单的节点实效的情况下，都会对事务的持久性产生影响。**基于提示的切换方法**是在节点失效情况下的数据更新传播和同步常用的方法。
	- 基于提示的切换方法 (hinted handoff)：当集群里的一个节点出现故障，数据自动进入临近的节点 handoff 区，收到恢复通知后自动恢复handoff数据。
		- 例如：假定系统中节点A和节点D是相邻节点，若节点A在执行写操作时暂时宕机或无法访问，那么，一个副本将被发送到节点D上。发送到D的副本在其元数据中包含称为hint的提示信息，hint的作用是提示原先哪个节点（在这种情况下为A节点）将负责接收副本。
		- 收到带有提示信息副本的节点（在这里是节点D）将把副本保存到本地一个单独的数据库中。一旦节点D监测到节点A恢复，D就会试图将副本传送给节点A，传送成功后，节点D再从其本地数据库中删除该副本。上述方法称为基于提示的切换。
		- 如果带有提示信息的副本在发送到其源节点（在上面的情况下是节点A）之前不可访问，那么就会出现存放副本的各节点数据不一致的情况。可以使用同步复制或Merkle Tree 的方法来解决。

# 主流NoSQL数据库

BigTable：Bigtable是一个分布式的结构化数据存储系统，它被设计用来处理海量数据：通常是分布在数千台普通服务器上的PB级的数据。
- Google的很多项目使用Bigtable存储数据：
	- 包括Web索引、Google Earth、Google Finance
	- 这些应用对Bigtable提出的要求差异非常大：
		- 无论是在数据量上（从URL到网页到卫星图像）
		- 还是在响应速度上（从后端的批量处理到实时数据服务）
	- 尽管应用需求差异很大，针对Google的这些产品，Bigtable还是成功的提供了一个灵活的、高性能的解决方案
- 设计动机：
	- 需要存储的数据种类繁多：Google目前向公众开放的服务很多，需要处理的数据类型也非常多。包括URL、网页内容、用户的个性化设置在内的数据都是Google需要经常处理的；
	- 海量的服务请求：Google运行着目前世界上最繁忙的系统，它每时每刻处理的客户服务请求数量是普通的系统根本无法承受的；
	- 商用数据库无法满足Google的需求：一方面现有商用数据库设计着眼点在于通用性，根本无法满足Google的苛刻服务要求；另一方面对于底层系统的完全掌控会给后期的系统维护、升级带来极大的便利
- Bigtable的设计的数据模型：
	- 行：
		- Bigtable的行关键字可以是任意的字符串，但是大小不能超过64KB。Bigtable和传统的关系型数据库有很大不同，它不支持一般意义上的事务，但能保证对于行的读写操作具有原子性（Atomic）
		- 表中数据都是根据行关键字进行排序的，排序使用的是词典序。
		- 一个典型实例，其中com.cnn.www就是一个行关键字。不直接存储网页地址而将其倒排是Bigtable的一个巧妙设计。这样做至少会带来以下两个好处
			- 同一地址域的网页会被存储在表中的连续位置，有利于用户查找和分析
			- 倒排便于数据压缩，可以大幅提高压缩率
	- 列：Bigtable并不是简单地存储所有的列关键字，而是将其组织成所谓的列族（Column Family），每个族中的数据都属于同一个类型，并且同族的数据会被压缩在一起保存。引入了列族的概念之后，列关键字就采用下述的语法规则来定义
		- 族名：限定词（family：qualifier）
			- 族名必须有意义，限定词则可以任意选定
			- 图中，内容（Contents）、锚点（Anchor）都是不同的族。而cnnsi.com和my.look.ca则是锚点族中不同的限定词
			- 族同时也是Bigtable中访问控制（Access Control）基本单元，也就是说访问权限的设置是在族这一级别上进行的
	- 时间戳：
		- Google的很多服务比如网页检索和用户的个性化设置等都需要保存不同时间的数据，这些不同的数据版本必须通过时间戳来区分。图2中内容列的t3、t5和t6表明其中保存了在t3、t5和t6这三个时间获取的网页。Bigtable中的时间戳是64位整型数，具体的赋值方式可以采取系统默认的方式，也可以用户自行定义
		- 为了简化不同版本的数据管理，Bigtable目前提供了两种设置：一种是保留最近的N个不同版本，图中数据模型采取的就是这种方法，它保存最新的三个版本数据。另一种就是保留限定时间内的所有不同版本，比如可以保存最近10天的所有不同版本数据。失效的版本将会由Bigtable的垃圾回收机制自动处理
- 系统架构
- Bigtable主要由三个部分组成：客户端程序库（Client Library）、一个主服务器（Master Server）和多个子表服务器（Tablet Server）
	- 客户访问Bigtable服务时，首先要利用其库函数执行Open()操作来打开一个锁（实际上就是获取了文件目录），锁打开以后客户端就可以和子表服务器进行通信
	- 和许多具有单个主节点分布式系统一样，客户端主要与子表服务器通信，几乎不和主服务器进行通信，这使得主服务器的负载大大降低
	- 主服务主要进行一些元数据操作以及子表服务器之间负载调度问题，实际数据是存储在子表服务器上
- Bigtable中Chubby的作用：
	- 选取并保证同一时间内只有一个主服务器（Master Server）
	- 获取子表的位置信息
	- 保存Bigtable的模式信息及访问控制列表
- 另外在Bigtable的实际执行过程中，Google的MapReduce和Sawzall也被用来改善其性能
- BigTable特点：
	- 适合大规模海量数据，PB级数据
	- 分布式、并发数据处理，效率极高
	- 易于扩展，支持动态伸缩
	- 适用于廉价设备
	- 适合于读操作，不适合写操作
	- 不适用于传统关系数据库

Dynamo：Dynamo 最初是 Amazon 所使用的一个私有的分布式存储系统
- 设计要点：
	- P2P 的架构：这区别于 Google FS 的 Single Master 架构，无须一个中心服务器来记录系统的元数据
- 技术要点：
	- 数据定位使用一致性哈希
	- Vector lock，允许数据的多个备份存在多个版本，提高写操作的可用性（用弱一致性来换取高的可用性）
	- 容错：Sloppy Quorum, hinted handoff, Merkle tree
		- Sloppy Quorum马虎仲裁，并非采用严格的数据一致性检查，用于实现最终一致性
		- hinted handoff，节点故障会恢复时，可动态维护系统可用性，使系统的写入成功大大提升。
		- 使用Merkle tree为数据建立索引，只要任意数据有变动，都将快速反馈出来。
	- 网络互联: Gossip-based membership protocol ，一种通讯协议，目标是让节点与节点之间通信，实现去中心化。
	- Storage load balancing
	- Client-driven Coordination
- 特点：
	- 高可用：设计上天然没有单点，每个实例由一组节点组成，从应用的角度看，实例提供 IO 能力。一个实例上的节点可能位于不同的数据中心内, 这样一个数据中心出问题也不会导致数据丢失
	- 总是可写：hinted handoff确保在系统节点出现故障或节点恢复时，能灵活处理
	- 可根据应用类型优化可用性、容错性和高效性配置
	- 去中心化，人工管理工作少
	- 可扩展性较差：由于增加机器需要给机器分配DHT(分布式hash table)算法所需的编号，操作复杂度较高，且每台机器存储了整个集群的机器信息及数据文件的Merkle Tree信息，机器最大规模只能到几千台

**Cassandra**：Apache Cassandra 是一套开源分布式NoSQL数据库系统。
- 它最初由Facebook开发，用于储存收件箱等简单格式数据，集Google BigTable的数据模型与Amazon Dynamo的完全分布式的架构于一身。
- Facebook于2008将 Cassandra 开源，此后，由于Cassandra良好的可扩放性，被Digg、Twitter等知名Web 2.0网站所采纳，成为了一种流行的分布式结构化数据存储方案。

HBase：Hadoop Database，是一个高可靠性、高性能、面向列、可伸缩的分布式存储系统，利用HBase技术可在廉价PC Server上搭建起大规模结构化存储集群
- HBase是Google Bigtable的开源实现，主要功能有：
	- 类似Google Bigtable利用GFS作为其文件存储系统，HBase利用Hadoop HDFS作为其文件存储系统
	- Google运行MapReduce来处理Bigtable中的海量数据，HBase同样利用Hadoop MapReduce来处理HBase中的海量数据
	- Google Bigtable利用 Chubby作为协同服务，HBase利用Zookeeper作为对应
- 数据模型：Row key、列族、列、时间戳，同bigtable定义

**Redis**：一个 key-value 存储系统
- 特点：
	- 和Memcached类似，它支持存储的value类型相对更多，包括string(字符串)、list(链表)、set(集合)和zset(有序集合)。
		- 这些数据类型都支持push/pop、add/remove及取交集并集和差集及更丰富的操作，而且这些操作都是原子性的。
		- 在此基础上，redis 支持各种不同方式的排序。
	- 与memcached一样，为了保证效率，数据都是缓存在内存中。区别的是redis会周期性的把更新的数据写入磁盘或者把修改操作写入追加的记录文件，并且在此基础上实现了master-slave(主从)同
- 相比memcached的优势：
	- value类型更丰富
	- 数据操作方法更多
	- 可将内存数据持久化

**MongoDB**：MongoDB是一个基于分布式文件存储的数据库。由C++语言编写。旨在为WEB应用提供可扩展的高性能数据存储解决方案。
- 特点：高性能、易部署、易使用，存储数据方便
	- MongoDB是一个介于关系数据库和非关系数据库之间的产品，是非关系数据库当中功能最丰富，最像关系数据库的。
	- 他支持的数据结构非常松散，是类似json的bjson格式，因此可以存储比较复杂的数据类型。
	- Mongo最大的特点是他支持的查询语言非常强大，其语法有点类似于面向对象的查询语言，几乎可以实现类似关系数据库单表查询的绝大部分功能，而且还支持对数据建立索引。
- 主要特点：
	- 面向集合存储：易存储对象类型的数据，包括文档内嵌对象及数组。
	- 模式自由：无需知道存储数据的任何结构定义，支持动态查询、完全索引，可轻易查询文档中内嵌的对象和数组
	- 文档型：存储在集合中的文档，被存储为键-值对的形式。键用于唯一标识一个文档，为字符串类型，而值则可以是各种复杂的文件类型。
	- 高效的数据存储：支持二进制数据及大型对象
	- 支持复制和故障恢复：提供Master-Master、Master-Slave模式的数据复制及服务器之间的数据复制
	- 自动分片：以支持云级别的伸缩性，支持水平的数据库集群，可动态添加额外的服务器

**Neo4j**
- **特点**：图数据库，支持复杂的关系查询。
- **应用场景**：社交网络分析、推荐系统、欺诈检测等。
