
人工神经网络 ANN
- 基础：感知器 (Perceptron)
- 东邪：基因组拼接
	- de Bruijn图算法
- 西毒：进化树分析
	- 近似算法和似然估计
- 南帝：基因和物种分类问题
	- 核函数
- 北丐：统计建模（序列分析）
- 中顽童：判断问题
	- 深度学习
	- 生成对抗神经网络GAN

# 机器学习

机器学习（Machine learning）
  - 发展历程：AI > Machine learning > Deep learning
	- 生信机器学习
		- SVM,
		- Artificial neural networks
		- Hidden Markov Model…
	  - 人工智能生物学（Artificial intelligence biology）
		  - 研究范式：大数据+ 大算力+ 强算法
	  - 中国：清华大学孙之荣教授:
		  - 1997年，将人工神经网络引入生物信息学领域
		  - 1999年，利用支持向量机预测可变剪接位点
		  - 2001年，构建蛋白质细胞亚定位预测工具SubLoc，以及蛋白质二级结构预测算法
- 数据类型：
	1. 序列数据：DNA、RNA、蛋白质序列
	2. 结构数据：NMR、X-ray、Cryo-EM/Cryo-ET
	3. 遗传/进化距离数据
	4. 谱数据：基因芯片、蛋白质-蛋白质相互作用
	5. 影像数据: 图像、视频（CT、MRI、超声）
	6. 文本数据：科学文献、电子病历
	7. 混合数据：二代测序数据（序列、谱）
- 研究模式
	- Model-based
	  需要建立假设
	  构建理论模型
	  不需要训练数据
	  不需要调整参数
	  准确性低
	  可解释性高
	  例如：基因芯片分析
	- Model-free
	  不需要建立假设
	  利用机器学习方法
	  需要训练数据
	  需要调整参数
	  准确性高
	  可解释性低
	  例如：蛋白质二级结构预测
  - 基本内容：A computer program is said to learn from
	- 学习对象
		- experience E with respect to some class of tasks T and performance measure P,
		- if its performance at tasks in T, as measured by P, improves with experience E.
	- 模型评估与选择
	  - 数据（样本/检验）
		- 阳性数据（P）
		  真实的，被实验所证实的数据
		- 阴性数据（N）
		  被实验所证明为无功能的数据
	  - 评估参数
		- 真阳性(TP)
		  阳性数据中被预测为阳性的数据
		- 假阳性(FP)
		  阴性数据中被预测为阳性的数据
		- 真阴性(TN)
		  阴性数据中被预测为阴性的数据
		- 假阴性(FN)
		  阳性数据中被预测为阴性的数据
	  - 评估方法
		评估算法准确性
		- 参数
		  - 灵敏度(Sensitivity, Sn)![img](https://api2.mubu.com/v3/document_image/d7bf4286-a080-464b-8af2-c185dedc4f37-12251550.jpg)
			对于真实的数据，能够预测成“真”的比例是多少-(Type II error)
		  - 特异性(Specificity, Sp)![img](https://api2.mubu.com/v3/document_image/3251aa4b-b5a4-4829-a27a-2eca0ac6f4b1-12251550.jpg)
			对于阴性的数据，能够预测成“假”的比例是多少-(Type I error)
		  - 准确性(Accuracy, Ac)![img](https://api2.mubu.com/v3/document_image/7b46fbfc-ce9e-4631-82f9-d488d21de373-12251550.jpg)
			对于整个数据集(包括阳性和阴性数据)，预测总共的准确比例是多少
		  - 马修相关系数(Mathew correlation coefficient, MCC)![img](https://api2.mubu.com/v3/document_image/2922e483-ee57-43d4-80cf-1f0a25621b64-12251550.jpg)
			当阳性数据的数量与阴性数据的数量差别较大时，能够更为公平的反映预测能力，值域[-1,1]
		  - ROC curve + AUC值
			X轴：1-Sp
			Y轴：Sn
			AUC（area under the curve）值：ROC的面积越大，预测能力越强
		- 预测性能的评估（反映预测性能）
		  - 自检法（Self-consistency validation）
			反映当前预测工具对目前已知的数据的预测能力​
			​训练数据当成测试数据
			训练数据中所有的阳性数据为测试数据中的阳性数据
			训练数据中所有的阴性数据为测试数据中的阴性数据
			- 假设：根据目前已知的数据所构建的计算模型能够反映未知的数据的模式
			- 缺点：不能反映计算模型的稳定性
		- 阈值确定
		  - Threshold 或 Cut-off
			依据经验，人为设定的一个阈值，阈值以上或以下预测为阳性，即利用阈值进行“一刀切”。
		  - 确定阈值的一般方法：
			传统策略：平衡Sn和Sp，使两者大致相当
			实际应用：高Sp低Sn保证预测结果的可靠性
			MCC最大值，保证综合预测性能最高
	  - 预测性能的检验（反映预测系统的稳定性）
		预测性能 vs. 检验性能
		差距较小：系统稳定
		差距过大：系统不稳定，数据过训练
		- 过训练
		  预测工具过训练：只能很好的符合训练数据，而对新数据则性能很差
		- 方法
		  - 除一法（Leave-one-out validation）
			每次从数据集中去掉一个，包括阳性数据和阴性数据
			利用剩下的数据重新训练，并构建新的计算模型
			对去掉的这一个数据进行打分
			保证每个数据去掉一次，从而得到所有数据的分值
			计算各个阈值的Ac, Sn, Sp和MCC
			计算AUC值，作为准确性
		  - N折交叉法（n-fold cross-validation）
			将数据集分成n组，并保证阳性数据与阴性数据的比例与原数据相同
			将n-1组作为训练数据，重新训练并构建计算模型；1组不用于训练
			将n-1组的数据重新分为n组，其中n-1组用来构建模型，1组用于调参
			对不用训练的1组进行打分，计算性能
			重复n次，使每组数据都用于独立测试集1次
			选取AUC最高模型
	- 线性模型 & 决策树
	  - 线性回归（linear regression）![img](https://api2.mubu.com/v3/document_image/c6752078-7937-4589-8c5c-0a3b86f6d05c-12251550.jpg)
	  - 对数几率回归/逻辑回归（logistic regression）![img](https://api2.mubu.com/v3/document_image/8d969770-682e-4f5c-b74e-ad09d2963dc4-12251550.jpg)
	  - 信息熵![img](https://api2.mubu.com/v3/document_image/65babae9-6afd-40fc-8292-1e1c0d7711f6-12251550.jpg)
	  - 信息增益![img](https://api2.mubu.com/v3/document_image/75e16ca1-d856-4e94-ad47-38d97376a839-12251550.jpg)
	- 神经网络 & 支持向量机
	  - 单层 & 多层神经网络
	  - 误差逆传播（Back propagation, BP）![img](https://api2.mubu.com/v3/document_image/df58a78c-423d-4fd2-b896-951e2187e197-12251550.jpg)
	  - 划分超平面![img](https://api2.mubu.com/v3/document_image/738a3df3-7ae9-4981-a221-b893087a8872-12251550.jpg)
	  - 核函数
	- 贝叶斯分类器 & 集成学习
	  - 贝叶斯最优分类器![img](https://api2.mubu.com/v3/document_image/ea365a12-9b73-4ad7-bc64-5d93fa79653c-12251550.jpg)
	  - 集成学习的错误率![img](https://api2.mubu.com/v3/document_image/25df0d51-cf11-4b44-ba0c-945efc2aeb67-12251550.jpg)
	  - 集成学习的方法
		- 并行化
		  Bagging、随机森林
		- 串行化
		  Boosting
	- 聚类 & 降维与度量学习
	  - 无监督学习
		- 性能度量
		- 距离计算
	  - 聚类分析方法
		- k-means clustering
		- Hierarchical clustering
	  - K近邻学习
	  - 低维嵌入
	  - 主成分分析
		t-SNE
	  - 将高维数据尽可能的投影到二维平面上
	- 概率图模型 & 规则学习 & 强化学习
	  - 隐马尔科夫模型（Hidden Markov Model, HMM）
	  - 发散概率可估算
	  - 状态转移态度未知（隐）
	  - 强化学习![img](https://api2.mubu.com/v3/document_image/0beb6422-dcd6-4816-af77-682312caf0cc-12251550.jpg)
		任务与奖赏
  - 机器学习常用软件包
	- Scikit-Learn
  - 深度学习（Deep learning）
	- Convolutional neural network, CNN![img](https://api2.mubu.com/v3/document_image/d3bac351-c2a5-4de4-b2f4-9db734e43213-12251550.jpg)
	- Deep neural network, DNN![img](https://api2.mubu.com/v3/document_image/d151405e-2a13-41ba-9c5e-0389c5a557e2-12251550.jpg)
	- Recursive neural network, RNN![img](https://api2.mubu.com/v3/document_image/847aec7b-9884-4ac6-9c86-01256665c67d-12251550.jpg)
	- Generative adversarial network, GAN![img](https://api2.mubu.com/v3/document_image/95035e06-a327-4c2a-8e9a-a9aee3dfd8c4-12251550.jpg)
	- 基本内容
	  - 神经元![img](https://api2.mubu.com/v3/document_image/73b4592b-43ff-4ade-b68f-7019d3d9ab1c-12251550.jpg)![img](https://api2.mubu.com/v3/document_image/46ec07ff-c0cb-4334-b0c4-f295b1f09267-12251550.jpg)
		神经元激活函数
	  - 卷积层![img](https://api2.mubu.com/v3/document_image/37f2a5fd-0830-4bcc-9a5e-5271dbfab143-12251550.jpg)
	  - 最大池化（Max pooling）![img](https://api2.mubu.com/v3/document_image/62e97e56-1b35-4816-ac73-b1677d0fd2c9-12251550.jpg)
	  - 输出![img](https://api2.mubu.com/v3/document_image/a92de5ca-95a7-47f0-a08e-1c2ca14e19cc-12251550.jpg)
	- 混合学习/融合AI![img](https://api2.mubu.com/v3/document_image/d2fa99aa-b697-437e-b00e-83598975d72f-12251550.jpg)
	- 软件包
	  - Keras
	- 和浅度学习对比![img](https://api2.mubu.com/v3/document_image/e159d36d-f440-45d6-bb11-8fdde2c77f18-12251550.jpg)

数学基础
- 生物序列的概率模型
	- 色子模型：一个色子存在6个概率值：p1, p2, …,p6，其中掷出i的概率为pi (i=1, 2, …, 6)。
		- 考虑三次连续的掷色子，结果为[1，6，3]，则总概率为：p1p6p3
	- 概率分布：f(x)称为概率密度函数
	- 二项分布：
		- 酵母全基因组复制（P值大奖）：基因数量的增加
			- 酵母~6000个基因，人类~21,000个基因
			- 单个基因复制、基因组复制、染色体片段复制
			- 通过基因组复制：K. waltii(克鲁雄酵母) > S. cerevisiae(酿酒酵母) 和S. bayanus(贝克酵母)
			- 克鲁雄酵母ORC1/SIR3：在酿酒酵母和贝克酵母中都有两个拷贝
		- 复制基因与已有基因的功能关系
			- “新功能形成”：Ohnoone-gene-only speeds-up (OS) model：一个基因功能不变从而进化慢，另一个需要产生新功能从而进化快
			- “亚功能形成”：Both-genes speed-up (BS) model：两个基因都只保留原有基因的部分功能，因此进化速率都快
		- 复制基因分别的进化速率估算
			- OS模型：其中一个基因进化速率快
			- BS模型：两个基因进化速率都快
		- 算p-value
			- 作者鉴定了酵母中457对通过全基因组复制产生的复制基因对（总共914个基因）。在酿酒酵母中，其中76对有加速进化的现象。“加速进化”在文中的定义指的是酿酒酵母里氨基酸替代率要比克鲁雄酵母里高50%。在76对有加速进化的复制基因对里，其中只有4对是两个基因都加速进化。因此基因对里只有一个加速进化的为72个基因（72/76=95%）
			- 统计模型
			  ![img](https://api2.mubu.com/v3/document_image/3191b256-7d4d-4f0f-b4cb-29d8615b4177-12251550.jpg)
	  - 泊松分布
		稀有事件发生的概率：在一个连续的时间或空间中，稀有离散变量出现的概率
		方差等于均值
		- 细菌对噬菌体的应答
		  进化：细菌是否有基因？受到噬菌体攻击如何生存？
		  拉马克机制：获得性遗传免疫假说–细菌在接触到噬菌体后，小概率产生抵抗，不需要基因或遗传物质
		  孟德尔机制：突变假说
		  - 细菌生存潜在机制
			- 两类机制
			  - 孟德尔——遗传变异
				细菌在噬菌体攻击之前已经具有抵抗能力，不需要与病毒相互作用，受到攻击时也不产生新的突变
				具有抵抗能力的细菌随时间比例增加
				- 非泊松分布：抵抗性细菌由紧密相关的个体构成群落
			  - 拉马克——获得性遗传免疫
				细菌在受到攻击的时候才产生免疫能力
				具有抵抗能力的细菌在受到攻击时的比例恒定
				只有当与病毒接触时才产生免疫
				- 泊松分布：每一个抵抗是一个独立的事件
			- 两类实验
			  有抵抗力的细菌比例是否随时间上升
			  观察细菌克隆的个数，看抵抗是否与遗传突变相关
			  - 将方差与均值进行比较
				在每一个实验中，可抵抗细菌的波动（fluctuation）远比均值高，不能归因于采样误差，与获得性遗传免疫的假设冲突
		- 鸟枪法的覆盖率（Lander-Waterman Model）
		  假设：需要测序的BAC长度200 kbp
		   总共测序的序列数量：N
		   每次测序：500 bp
		   每次测序的覆盖率p：500/200 kbp=0.0025
		   因此：总覆盖率C=Np（每个点平均覆盖到的次数）
		  Y: 测序能够覆盖到点X的次数
		  - 近似符合泊松分布（Poisson distribution）
			- 准确性![img](https://api2.mubu.com/v3/document_image/c7da266e-b3c1-495c-95f2-f9b589c09b43-12251550.jpg)
		- 随机产生多个基因的概率![img](https://api2.mubu.com/v3/document_image/4a67de48-bed3-4b7f-885e-d72f016cf04e-12251550.jpg)
	  - 超几何分布
		与二项式分布的区别：不放回抽样
		- 与二项分布对比![img](https://api2.mubu.com/v3/document_image/9359f24c-c335-47dc-902d-6aa4d13b4a61-12251550.jpg)![img](https://api2.mubu.com/v3/document_image/c55c8c1c-a624-4ba1-aca4-de29d2d17f49-12251550.jpg)
		- 不等式（p-value）![img](https://api2.mubu.com/v3/document_image/d111de14-9ca1-4464-8d9c-40db9d051626-12251550.jpg)![img](https://api2.mubu.com/v3/document_image/b7fadf1f-8a8d-4491-b496-163cd7900ccd-12251550.jpg)
		- 例
		  研究者从26873个人类蛋白质中预测了2264个具有某种特定功能的底物，并进行进一步的分析。其中，有421个人类蛋白质具有某种功能结构域D，而在预测的2264个底物中，有94个蛋白质具有结构域D
		  ​问：结构域D在2264个底物中是显著出现，显著不出现，还是随机出现？
		  - 问题描述![img](https://api2.mubu.com/v3/document_image/25d72b06-9998-41eb-822c-aee41b33f3f3-12251550.jpg)
			N = 26873; n= 2264; M = 421; m = 94;
			(m/n)/(M/N) = 2.65
			因此，问题转化：在26873个人的蛋白质中，抓出2264个蛋白质，其中至少有94个蛋白质具有功能结构域的概率是多少？
		  - 统计显著性
			统计显著：p-value < 0.05
			考虑两个假设H0（空假设）和H1（备择假设）
			H0代表随机情况下事件出现的概率
			H1代表当前出现事件的概率
			如果H0/H1<< 0.05，则接受H1而不接受H0
			超几何分布的p-value：
			“完全随机状态下”事件出现的概率，即p-value=H0
			H1=1
			- 超几何分布的精确概率计算：
			  2X2表
			  - 计算公式（Fisher’s Exact Test）
				- 上例
				  a+b+c+d=26873,
				  c+d=2264,
				  b+d=421,
				  d=94
	- 随机序列模型：假设一个残基a随机出现的概率为qa，并且该概率独立于其它残基而存在
		- 生物序列![img](https://api2.mubu.com/v3/document_image/f12a8f3c-f614-4b8c-a8d0-81821995f46a-12251550.jpg)
	- 最大似然估计
	  - 概率模型的参数通常是从大的可靠的数据集，即训练集中估算得到
		例如：通过对Swissprot数据库分析，各个物种中，20种氨基酸出现的频率
		- 最大似然性估计：
		  充分使用了训练集的数据，作为概率模型的估算参数​
		  一般的，给定一个模型，包括参数θ以及数据集D，则对于参数θ的最大似然性估计，要保证P(D|θ)的最大化
		  - 过训练（over-fitting）
			例如：掷色子3次，得到[6，6，6]，根据最大似然性的模型，则p1=p2=p3=p4=p5=0; p6=1
		- 氨基酸频率（几个主要真核物种中）![img](https://api2.mubu.com/v3/document_image/e0aaf692-e5ee-4cfd-9272-783daebb15ec-12251550.jpg)
	- 思路流程
	  - 不同种概率
		考虑两个色子D1和D2
		- 条件概率：
		  用色子D1掷出i的概率为P(i|D1);用色子D2掷出i的概率为P(i|D2)
		- 连接概率：
		  随机挑出一个色子的概率P(Dj), j=1,2; 挑到第j色子且掷出一个i的概率（条件概率）为：P(i,Dj)= P(Dj)P(i|Dj)。一般定义为：P(X,Y) = P(X|Y)P(Y)
		- 边际概率：![img](https://api2.mubu.com/v3/document_image/a6a7f301-0cb9-40d5-aa52-02232465862e-12251550.jpg)
		  当条件或者连接概率已知的时候，可以计算边际概率并去掉一个变量
	  - 故事
		某天，Prof. Gene来到拉斯维加斯去旅游，一时兴起，就去了一个赌场玩两把。游戏是掷色子。但是，据说这个赌场的荷官不老实，使用了两种色子，其中99%的色子是正常（fair）的，而1%的色子（loaded）则使得出现6的概率为50%
	  - 问题
		那么，P(6|Dloaded)和P(6|Dfair)是多少？而P(6,Dloaded)和P(6,Dfair)呢？随机拿到一个色子掷出6的概率是多少？
	  - 可能性及结果
		P(6|Dloaded)=0.5
		P(6|Dfair)=1/6
		P(6,Dloaded)=0.5* 0.01=0.005
		P(6,Dfair)=(1/6)* 0.99
		随机拿到一个色子掷出6的概率：P(6,Dloaded) + P(6,Dfair)
	  - 新问题
		Prof. Gene拿起一个色子，连续掷了三次，都是6，因此，他判断这个色子是loaded。他这样的判断可靠吗？如果不可靠，那么，怎样才能判断色子可能是loaded呢？
	  - 贝叶斯理论及模型比较
		前向概率（prior probability）：P(Dloaded)=0.01 和P(Dfair)=0.99
		后向概率（posterior probability）:P(Dloaded|3个6)
		- 结果![img](https://api2.mubu.com/v3/document_image/ff841f15-333d-4233-b936-bd7b800740e3-12251550.jpg)
	  - 扩展
		怎样才能认为是loaded色子？
		P(Dloaded|n个6)
		四个6：P=0.45
		五个6：P=0.71>0.5
		当连续掷出5个6以上时，我们认为可能是loaded！
		…
	- 例子
	  - 对于给定4 bp的序列
		- 对于ACTG序列
		  - 编程解决
			- 结果
			  P(X-box|ACTG)=0.91！
			  P(X-box|ATTT)=0.08
			  P(X-box|AGTG)=0.60
			  P(X-box|CCGA)=0.0009!

## 深度学习的应用

监督学习

- 分类
	- 简单贝叶斯
	- SVM
- 神经网络
	- 输入层
	- 中间 多层神经网络

无监督学习

- 聚类

### 概貌

机器学习过程：训练 – 测试 – 模型(一个映射函数)
- 三要素：因为机器学习要得到模型来预测未知的问题，可以看到机器学习合传统优化问题有区别
	- 模型：线性/非线性
	- 学习准则：期望风险
	- 优化：梯度下降 – 收敛到局部最优
- 类型：
	- 训练样本
	- 优化目标
	- 学习准则

深度学习：训练层数变得更多
- 层数 – 分元素矩阵变大 – 组合找到特征 – 结合
- 应用：图像领域、语音识别、视频
- 对抗学习
- 神经网络：层次网络结构 – 神经元(输入和输出)
	- 逐层优化，逐层初始化

迁移学习 – 基于深度学习 – 将猫狗的识别模型迁移到人身上

### 基本方法

自然对数

logistic激活函数，sigmoid

逻辑回归：

- 学习效率 – 每次调整的角度

神经网络：

- 输入层
- 隐藏层 – 深度学习多隐藏层 – sigmoid函数
	- 残差反向来传 – 神经网络学习：反向传播
	- 传递之间
		- 卷积(convolution)：把一个大的矩阵变成小的矩阵 – 重叠
			- 特征 – 左小右大，上下相同
			- 整体上特性的一致
		- 驰化(pooling) – 不重叠
			- 最大值驰化
- 输出层

人工神经网络(ANN)

卷积神经网络(CNN)

对抗神经网络(GAN)

强化学习

## 生物大数据挖掘的深度学习

分类 – 有监督

聚类 – 无监督

强化学习 – 有奖赏的

one-hot

对高保守序列 – 可以用强化学习来找特征

- HMM
- BLAST

建模 – 提取特征 – 建立网络

## 习题
### 概念

一、对于真核基因剪接位点识别这一问题，假设依据的信息为位点前后共计d个位置上的碱基序列，请回答

1. 若采用朴素贝叶斯方法，给出一种判别函数并说明需要确定的参数及其个数（4分）

   - 各位点概率相乘，共**4* d* 2+2**个参数

2. 若采用理想贝叶斯方法，给出一种判别函数并说明需要确定的参数及其个数（5分）

   - ln(前验概率* 条件概率密度)，共**4^d* 2+2**个参数

3. 比较上述两种方法的优缺点并给出一种与之不同的贝叶斯方法（10分）

   - 半朴素贝叶斯

4. 若采用 WMM 方法，给出一种判别函数并说明需要确定的参数及其个数（5分）

   - ln(位点概率/背景概率)，共**4* d* 2**个参数

5. 若采用 WAM 方法，给出一种判别函数并说明需要确定的参数及其个数（5分）

   - 共**[4^2* (d-1)+4]* 2**

6. （4）以上三种方法得到的预测效果会是什么关系，为什么？（6分）

   - 根据特征之间关系的考虑程度：

   - 理想贝叶斯>SVM、HMM、ANN>贝叶斯网络>WAM>朴素贝叶斯、WMM

7. 比较上述方法的优缺点并给出一种与之不同的贝叶斯方法（8分）

   1. WMM
      1. 优点：方法简单，涉及的参数少，便于计算和训练；
      2. 缺点：将每个位置出现碱基的概率都考虑为独立的，未考虑之间的相关性，不太符合实际。
   2. WAM
      1. 优点：考虑到相邻碱基之间的依赖性，引入条件概率分布，方法相对简单，涉及的参数也不是很多；
      2. 缺点：仅仅考虑相邻碱基之间的依赖性，忽略了长程关联，在预测时效果也不会太理想。
   3. 理想贝叶斯方法
      1. 优点：考虑到特征之间可能存在的所有依赖性的关系，并将其每种组合都列出概率分布，误差小，预测性能好；
      2. 缺点：参数太多，导致后验概率种类过多，数据量庞大，计算不便。
   4. 另可采用朴素贝叶斯方法，原理与理想贝叶斯相似，但在处理条件概率分布时认为给定类别下特征向量是条件独立的，即P(*x*1,…,*xd*|ω*i*)=∏*s*=1,..,d P(*xs*|ω*i*)，

一、给出基于最小错误率规则的两种理想贝叶斯决策函数；简述理想贝叶斯分类方法的特点与关键问题；分析WMM、WAM方法与理想贝叶斯方法的关系。（20分）

1. 特点：理论上，贝叶斯分类器具有最优的性能，即所实现的分类错误率或风险在所有可能的分类器中是最小的。但贝叶斯分类要求具备以下前提条件：类别数目已知，各个类别的先验概率以及类条件概率密度均为已知（必须是真实情况）；
2. 关键问题：对于高维特征x=[x1,…,xd]，贝叶斯定理中的似然函数（类条件概率密度或概率分布）P(x|ωi) = P(x1,…,xd|ωi)是一个高维分布，如何有效确定该函数是实现贝叶斯分类的关键问题。
3. WMM、WAM方法是理想贝叶斯方法的简化版本，具体来说，WMM是理想贝叶斯最简化的版本，完全不考虑每个特征之间的联系；WAM也是理想贝叶斯简化的版本，但是考虑了相邻特征之间的联系。

**一、**对于贝叶斯分类，请简要回答以下问题：

1. 贝叶斯分类的基本思想是什么？（5分）

   1. 已知类条件概率密度参数表达式和先验概率
   2. 利用贝叶斯公式转换成后验概率
   3. 根据后验概率大小进行决策分类

2. 叶斯分类的关键问题是什么? （5分）

   对于高维特征x=[x1,…,xd]，贝叶斯定理中的似然函数（类条件概率密度或概率分布）P(x|ωi) = P(x1,…,xd|ωi)是一个高维分布，如何有效确定该函数是实现贝叶斯分类的关键问题。

3. 理想贝叶斯、朴素贝叶斯和贝叶斯网络方法各有何优点？（10分）

   1. 理想贝叶斯：理论上，贝叶斯分类器具有最优的性能，即所实现的分类错误率或风险在所有可能的分类器中是最小的
      1. 但贝叶斯分类要求具备以下前提条件：类别数目已知，各个类别的先验概率以及类条件概率密度均为已知
   2. 朴素贝叶斯：参数少，可以实现，简洁
      1. 但是效果不佳
   3. 贝叶斯网络：有用的参数、关系就保留，无用的参数就丢弃，效果会增加；

### SVM

二、对于上题中的问题，现改用支持向量机（ SVM ）方法建立预测模型，请简要回答

1. 线性支持向量机的优化目标是什么？（5分）

   对n个观测样本，找到最优的分类平面

2. 核函数的作用是什么，核函数如何选择？核函数的参数如何确定？（6分）

   将样本映射到新的特征空间，实现在判别函数中的分离

3. 影响支持向量机方法预测效果的主要因素有哪些？（6分）

   训练数据、核函数类型

4. 支持向量机的复杂度取决于什么？

   支持向量和错分的个数，即不为0的α的个数；

二．请设计采用SVM方法识别Donor位点的一个完整方案

1. Selection of Features 特征选择：Fragments of 12 bps around TIS 
2. Data Sets 设置数据集
3. Encoding the sequence 编码序列： A:(1,0,0,0); G:(0,1,0,0); C:(0,0,1,0); T:(0,0,0,1)
4. Selecting Kernel function 选择核函数
   1. 核函数作用：避免了直接在新的超高维平面设计支持向量机的麻烦；做了一个隐含的映射，避免了显性的映射；
   2. 类型和参数的确定：尝试，选择效果最好的
5. Designating Parameters 设计参数

实际应用中支持向量机方法的潜力主要受哪些因素制约？

- 对大规模训练样本难以实施、解决多分类问题存在困难

**二．**请根据“利用支行向量机方法预测真核基因剪接位点”这一综合训练内容简要回答以下问题： 

1.  (1) 所采用的特征向量的维数是多少（4分）

   碱基分布：4

2.  (3) 线性支持向量机的优化目标（准则函数）是由哪两部分组成的？（4分）

   第一部分是经验风险最小；第二部分是推广能力最大

3.  (4) 支持向量机方法解决线性不可分问题的思路是什么？（4分） 

   映射到更高维的特征空间，使其可分。

4.  (5) 支持向量的个数与用于训练支持向量机的样本数之间是什么关系？（4分） 

   支持向量的个数远小于用于训练支持向量机的样本数

### ANN

三、对于上题中的问题，现采用单隐层前馈神经网络建立预测模型，请简要回答

1. （1）模型的输入层和输出层分别有多少个节点？（5分）

   输入层有d个，输出层有1个

2. （2）隐层的作用是什么？隐层节点数如何确定？（7分）

   1. 作用：为了是模型能够执行和学习任何功能，对输入数据进行处理。降低网络误差（也有文献认为不一定能有效降低），提高精度，但也使网络复杂化
   2. 确定节点数：针对训练样本数确定，必须小于N-1，与需解决的问题的复杂程度和转换函数的型式以及样本数据的特性等因素有关。在满足精度要求的前提下取尽可能紧凑的结构，即取尽可能少的隐层节点数。

3. （3）该模型与上一题的 SVM 模型有哪些相同之处和不同之处？两者的预测准确性可能会是什么关系？（8分）

   1. 相同：都是映射，都属于浅层机器学习模型
   2. 不同：显性和隐性
      1. 人工神经网络是以传统统计学为基础，其研究的内容是样本数据趋于无穷多时的统计性质，适用于超大样本数据，且可轻松从二分类拓展到多分类；
      2. 支持向量机则是以统计学习理论为基础,不涉及概率测度及大数定律等，计算的复杂性取决于支持向量的数目，避免了“维数灾难”，克服了神经网络中的“过学习问题”。更适用于小样本数据，且在多分类问题上不能直接运用。

4. 实际应用中人工神经网络方法的潜力主要受哪些因素制约？

   1. 易出现“过拟合”、易陷入局部最优

### HMM

四、对于隐马尔科夫模型（ HMM ）方法，请间要回答

1. HMM 的建立与应用需要解决哪些问题？（5分）
   1. HMM的结构
   2. 估计问题：根据一个O，建立一个M，在给定M下评估该序列的概率
   3. 解码问题：找到O最可能通过M的路径Q
   4. 学习问题：根据给出的O，来对M进行优化，其中可能存在Q。确定模型M参数
2. 给出采用 HMM 方法预测蛋白质二级结构的一种模型拓扑结构，并给出该模型需要确定的参数类型与个数。（10分）
   1. 有helix、coil、strand三种状态，根据序列编码特征向量，先进行训练而后在交叉检验后，使用模型对给定蛋白质的二级结构进行预测
   2. T: 3* 20* 3=180; E: 20* 3* 20=1200，共1380个参数

请根据以下一组保守序列片断的多序列比对结果，给出：

1. 描述该保守序列片断的隐马尔可夫模型（HMM）结构图；
2. 该HMM的状态转移概率矩阵（Transition matrix）和生成概率矩阵（Emission matrix）。（15分）

四．对于如图（A）和图（B）所示的两组保守序列片断的多序列比对结果，分析各适合用什么模型描述，并简要说明理由（15分）

1. 图A：有空格，隐马尔可夫模型，存在若干个不可观测的隐状态；
2. 图B：无空格，WAM或者WMM模型或SVM或贝叶斯网络，不存在隐状态；

### 特征向量

五、当采用支持向量机或人工神经网络等方法建立分类模型对蛋白质序列的功能进行分析时，需要先选取某些特征将长度任意的蛋白质序列编码成固定长度的特征向量。请设计一种能综合考虑各氨基酸的组成情况及其在序列长度上的分布情况的编码方案，并给出最终所得到的特征向量的维数。（15分）

1. 组成：**C=20**
2. 转换：**T=20* 20**
3. 分布：**D=20* 5* 4**
4. 共500维

给出考虑氨基酸组成及其疏水性（三种）将长度任意的蛋白质序列编码为固定长度特征向量的一种方案，并给出该特征向量的维数（15分）

1. C：3 
2. T：3* 3=9 
3. D：4* 3=12（四等分）
4. 24
5. 加上氨基酸组成，共524维

### 论述题

六、如果遇到类似真核基因剪接位点识别这样的研究课题，你认为应从哪些方面开展研究。（10分）

1. 6-mer等

五．对于Donor位点预测这一问题，目前你已了解了WMM、WAM、贝叶斯网络、SVM、ANN等方法

1. 你认为这些方法的预测准确度是一个什么顺序？（5分）
   1. SVM、HMM、ANN>贝叶斯网络>wam>wmm
2. 为了进一步提高预测性能，你认为应从哪些方面开展工作。（15分）
   1. 数据，质量，数据量，优化数据集的质量，保证原始数据的正确性。
   2. 特征之间的关系，有用的，优化特征的选取。对于深度学习来说，自动选取特征是难以实现的。对于图像识别来说，原始特征就是原始数据。
   3. 更有效的先验知识，虽然关注的是donor位点，但可以站在更高的层次去研究，可以站在编码区，基因结构的层面去研究。
   4. 更强的计算能力
   5. 更好的模型结构，只考虑有用的，模型的选择：选取donor位点的模型有很多，但是没有理论告诉我们哪种模型最好。可以尝试多种模型以及多种模型的结合

六．简要总结：

1. 你在本课程上机实践训练中所完成的内容、遇到了哪些困难、思考了哪些问题；（10分）
   1. 真核基因剪接位点预测的WAM模型编程实现中：
      1. 完成了：特征的提取（选取剪接位点前3个和后4个位置碱基信息）、训练集数据的提取和表示，统计算出WAM条件概率模型（包括P+和P-，用二维数组表示）、设置初始C值对测试集数据依次滑动进行打分、计算Sn和Sp、修改C值并作出了曲线，从图中选取Sn、Sp较理想的C值
      2. 遇到的困难：如何批量读取文件并将其中的数据输入到二维数组中
   2. 利用SVM方法预测真核基因剪接位点中：
      1. 完成了特征提取与数据编码、核函数的选择（高斯核）、参数学习
      2. 遇到的困难：核函数的选取和参数的确定
2. 通过本课程的学习与训练你有哪些收获。（5分）
   1. 了解和学习了WMM、WAM、贝叶斯网络、理想贝叶斯、SVM、HMM、传统ANN、深度神经网络等数据挖掘方法，明白了这些方法的基本原理以及能逐步应用这些方法去解决一些生物信息学研究中的问题。
   2. 分析问题和解决问题的能力得到了提升，对我们生信专业有了更深层次的认识。
